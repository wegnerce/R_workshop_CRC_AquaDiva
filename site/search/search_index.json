{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"From Excel to R Preliminary stuff seq(x,y,z) # creates a vector with the numbers from x to y in step size z, e.g. seq(0,30,5) is equivalent to c(0,5,10,15,20,25,30) Import data from excel via the clipboard df - read.table(file= clipboard ,sep= \\t ,header=TRUE,row.names=1, dec= , ) # file= source of the data, in our case the clipboard # sep= separator between data points, in our case \\t for tabulator-separated # header= does the dataset contain headers (column names)? TRUE or FALSE # row.names= in which column are names of the rows stored? Here 1 for first column, leave away for no row names # dec= sets the decimal separator, either , or . Set the margins of the plot par(mfrow=, mar=, oma=) # warning! par sets global parameters, if you change anything in par, it will stay changed until you set it back to the default # all parameters are optional, you can change one or multiple ones at a time # mfrow= create multi plot figures; requires a vector of 2, c(2,3) will arrange plots in 2 rows and 3 columns; default is mfrow=c(1,1) # mar= margins around plot in lines, requires a vector of 4, the order is bottom, left, top, right; default is c(5.1,4.1,4.1,2.1) # oma= outer margins for multi plot figures; default is c(0,0,0,0) # if someting seems messed up, run the line below par(mfrow=c(1,1), mar=c(5.1,4.1,4.1,2.1), oma=c(0,0,0,0)) Make a basic x-y-plot plot(x=, y=, xlim=c(0,1), ylim=c(0,1), xaxs= i , yaxs= i , xaxt= n , yaxt= n , xlab= , ylab= , frame.plot=FALSE, main=) # x=, y= vectors of x and y values; supply only x for categorial values; x=NULL creates an empty plot # all other parameters are optional # xlim=, ylim= vectors of length 2 specifying the axis ranges, e.g. xlim=c(0,10) for an x axis from 0 to 10 # xaxs=, yaxs= either i or r , with r the axis extends 4% beyond the values specified in xlim/ylim, with i not; default is r # xaxt=, yaxt= with n , no axis is drawn, so the axis can be specified with the axis() function # xlab=, ylab= title for the axis, e.g. Length in meters or for no title # frame.plot= draw a frame around the plot, TRUE or FALSE # main= string for a plot title, e.g. My first plot # see ?par for more parameters that can be used in plot() and axis(), e.g. regarding line colors, text size, style, color and font Draw an axis axis(side=, at=, labels=, tck=, lwd=, lwd.ticks=, las=) # side= specifies which axis to draw, 1 bottom x-axis, 2 left y-axis, 3 top x-axis, 4 right y-axis # at= vector that specifies where ticks and tick labels should be, best use as at=seq() (see above) # labels= vector with strings to be drawn at the at= positions; should be same length as at=; not required for numbers # tck= optional; specifies tick length and direction, default is tck=-0.02 (short ticks to the outside) # lwd= optional;specifies the line width of the axis line, default is lwd=1 # lwd.ticks= optional; specifies the line with of the tick lines, default is lwd.ticks=1 # las= optional; orientation of the tick labels, las=1 is horizontal, las=3 is vertical Draw grid lines or other kind of lines (not between datapoints though) abline(a=, b=, h=, v=, coef=, lwd=, col=, lty=) # best only use a b or h or v or coef # a=, b= intercept and slope for a line, e.g. a=1, b=2 for intercept 1, slope 2 # h= coordinates for horizontal lines, e.g. for an axis with at=seq(0,1,0.2), this can be h=seq(0,1,0.2) # v= same as h but for vertical lines # coef= as a and b, but intercept and slope are in a vector, e.g. coef=c(1,2) will produce the same line as above # lwd= optional; see axis # col= optional; specify the color of the line # lty= optional; line type, 1 solid, 2 dash, 3 dot, 4 dot dash etc. etc. Plot points points(x=, y=, pch=, cex=, col=, xpd=) # x=, y= see plot # pch= optional; kind of symbols to draw, run next line of code to see pch=0 to pch=25 plot(rep(0,25),pch=0:25) # cex= optional; size of the symbol, default is 1 # col= optional; color of the symbol # xpd= optional; draw symbols outside of plot area? TRUE or FALSE, default is FALSE Plot lines between points lines(x=, y=, lwd=, col=) # x=, y= see plot, lines will be drawn between the neighboring points # lwd= optional; line width; default is 1 # col= optional; color of the line Add a legend legend(x=, y=, legend=, bty= n , pch=, pt.cex=, lwd=, col=, y.intersp=, xpd=) # x=, y= position of the legend based on the axis of the plot; can be replaced by keywords like topright for the top right corner of the plot # legend= vector of text strings for each legend entry # bty= optional; box around the legend, bty= n for no box # pch= optional; draw points in the legend, see points() # pt.cex= optional; size of the points; default is 1 # lwd= optional; specify width for a line to be drawn in the legend; leave out for no line # col= optional; vector of colors for the points and lines # y.intersp= optional; line spacing in the legend; default is 1 # xpd= optional; draw symols outside of plot area? TRUE or FALSE, default is FALSE Add error bars to points (example for standard dev SD of y-error-bars) this is a quick way using a function that draws arrows arrows(x0=, y0=, x1=, y1=, length=, angle=90, code=3, col=) # if x and y are the vectors you used to plot points(), then: # x0=, y0= vectors for coordinates FROM which to draw bars, e.g. x0=x, y0=y-SD # x1=, y1= vectors for coordinates TO which to draw bars, e.g. x1=x, y1=y+SD # length= length of the horizontal lines at the ends of the error bar, default is 0.25 # angle= angle of the horizontal lines at the ends of the error bar, leave at 90 # code= draw horizontal lines at the top, bottom or both sides of the error bar, leave at 3 # col= color of the error bars Add other lines of text to your plot mtext(text=, side=, line=, font=) # text= string of text to print in the plot # side= at which side of the plot and in which orientation the text should be printed, side=1 bottom, =2 left, =3 top, =4 right # line= on which line the text should be printed, = 0 means outside of the plot, 0 means inside of the plot # font= style of the text; =1 plain, =2 bold, =3 italic, =4 bold italic # for superscript and subscript, text=expression() can be used; run the lines of code below to see an example plot(NULL, xlim=c(0,10), ylim=c(0,5), xaxs= i , yaxs= i , xaxt= n , yaxt= n , xlab= , ylab= , frame.plot=FALSE) mtext(text=expression( Normal text [ Subscript text ]* normal again ^ superscript * normal ), side=3, line=-3) mtext(text=expression( For numbers, this also works without quotation marks [420]*123^-2), side=3, line=-5) mtext(text=expression( This can also do that [ bottom ]^ top ), side=3, line=-7) Bar chart barplot(height=, space=, names.arg=, beside=, horiz=, col=, border=, axes=, axisnames=) # height= vector or matrix of values for the height of the bars; in a matrix, the values in each column will be stacked into one bar # when using a dataframe called df as input, use as.matrix(df); you might also need to transpose the matrix with t(as.matrix(df)) # space= optional; distance between the bars; default is 0.2 # names.arg= optional; vector of strings for bar names to be drawn below x axis; default is NULL # beside= optional; TRUE or FALSE, if TRUE, values from columns are drawn next to each other and not stacked; default is FALSE # horiz= optional; TRUE or FALSE, if TRUE, bars are drawn horizontally; default is FALSE # col= optional; vector of colors for bars, row by row # border= optional; TRUE or FALSE, if TRUE, a box is drawn around each bar; default is TRUE # axes= optional; TRUE or FALSE, if TRUE, the y axis is drawn; default is TRUE # axisnames= optional; TRUE or FALSE, if TRUE, the names of the bars (see names.arg=) are drawn; default is TRUE Written by Martin Taubert Oct 2018","title":"From Excel to R"},{"location":"#from-excel-to-r","text":"","title":"From Excel to R"},{"location":"#preliminary-stuff","text":"seq(x,y,z) # creates a vector with the numbers from x to y in step size z, e.g. seq(0,30,5) is equivalent to c(0,5,10,15,20,25,30)","title":"Preliminary stuff"},{"location":"#import-data-from-excel-via-the-clipboard","text":"df - read.table(file= clipboard ,sep= \\t ,header=TRUE,row.names=1, dec= , ) # file= source of the data, in our case the clipboard # sep= separator between data points, in our case \\t for tabulator-separated # header= does the dataset contain headers (column names)? TRUE or FALSE # row.names= in which column are names of the rows stored? Here 1 for first column, leave away for no row names # dec= sets the decimal separator, either , or .","title":"Import data from excel via the clipboard"},{"location":"#set-the-margins-of-the-plot","text":"par(mfrow=, mar=, oma=) # warning! par sets global parameters, if you change anything in par, it will stay changed until you set it back to the default # all parameters are optional, you can change one or multiple ones at a time # mfrow= create multi plot figures; requires a vector of 2, c(2,3) will arrange plots in 2 rows and 3 columns; default is mfrow=c(1,1) # mar= margins around plot in lines, requires a vector of 4, the order is bottom, left, top, right; default is c(5.1,4.1,4.1,2.1) # oma= outer margins for multi plot figures; default is c(0,0,0,0) # if someting seems messed up, run the line below par(mfrow=c(1,1), mar=c(5.1,4.1,4.1,2.1), oma=c(0,0,0,0))","title":"Set the margins of the plot"},{"location":"#make-a-basic-x-y-plot","text":"plot(x=, y=, xlim=c(0,1), ylim=c(0,1), xaxs= i , yaxs= i , xaxt= n , yaxt= n , xlab= , ylab= , frame.plot=FALSE, main=) # x=, y= vectors of x and y values; supply only x for categorial values; x=NULL creates an empty plot # all other parameters are optional # xlim=, ylim= vectors of length 2 specifying the axis ranges, e.g. xlim=c(0,10) for an x axis from 0 to 10 # xaxs=, yaxs= either i or r , with r the axis extends 4% beyond the values specified in xlim/ylim, with i not; default is r # xaxt=, yaxt= with n , no axis is drawn, so the axis can be specified with the axis() function # xlab=, ylab= title for the axis, e.g. Length in meters or for no title # frame.plot= draw a frame around the plot, TRUE or FALSE # main= string for a plot title, e.g. My first plot # see ?par for more parameters that can be used in plot() and axis(), e.g. regarding line colors, text size, style, color and font","title":"Make a basic x-y-plot"},{"location":"#draw-an-axis","text":"axis(side=, at=, labels=, tck=, lwd=, lwd.ticks=, las=) # side= specifies which axis to draw, 1 bottom x-axis, 2 left y-axis, 3 top x-axis, 4 right y-axis # at= vector that specifies where ticks and tick labels should be, best use as at=seq() (see above) # labels= vector with strings to be drawn at the at= positions; should be same length as at=; not required for numbers # tck= optional; specifies tick length and direction, default is tck=-0.02 (short ticks to the outside) # lwd= optional;specifies the line width of the axis line, default is lwd=1 # lwd.ticks= optional; specifies the line with of the tick lines, default is lwd.ticks=1 # las= optional; orientation of the tick labels, las=1 is horizontal, las=3 is vertical","title":"Draw an axis"},{"location":"#draw-grid-lines-or-other-kind-of-lines-not-between-datapoints-though","text":"abline(a=, b=, h=, v=, coef=, lwd=, col=, lty=) # best only use a b or h or v or coef # a=, b= intercept and slope for a line, e.g. a=1, b=2 for intercept 1, slope 2 # h= coordinates for horizontal lines, e.g. for an axis with at=seq(0,1,0.2), this can be h=seq(0,1,0.2) # v= same as h but for vertical lines # coef= as a and b, but intercept and slope are in a vector, e.g. coef=c(1,2) will produce the same line as above # lwd= optional; see axis # col= optional; specify the color of the line # lty= optional; line type, 1 solid, 2 dash, 3 dot, 4 dot dash etc. etc.","title":"Draw grid lines or other kind of lines (not between datapoints though)"},{"location":"#plot-points","text":"points(x=, y=, pch=, cex=, col=, xpd=) # x=, y= see plot # pch= optional; kind of symbols to draw, run next line of code to see pch=0 to pch=25 plot(rep(0,25),pch=0:25) # cex= optional; size of the symbol, default is 1 # col= optional; color of the symbol # xpd= optional; draw symbols outside of plot area? TRUE or FALSE, default is FALSE","title":"Plot points"},{"location":"#plot-lines-between-points","text":"lines(x=, y=, lwd=, col=) # x=, y= see plot, lines will be drawn between the neighboring points # lwd= optional; line width; default is 1 # col= optional; color of the line","title":"Plot lines between points"},{"location":"#add-a-legend","text":"legend(x=, y=, legend=, bty= n , pch=, pt.cex=, lwd=, col=, y.intersp=, xpd=) # x=, y= position of the legend based on the axis of the plot; can be replaced by keywords like topright for the top right corner of the plot # legend= vector of text strings for each legend entry # bty= optional; box around the legend, bty= n for no box # pch= optional; draw points in the legend, see points() # pt.cex= optional; size of the points; default is 1 # lwd= optional; specify width for a line to be drawn in the legend; leave out for no line # col= optional; vector of colors for the points and lines # y.intersp= optional; line spacing in the legend; default is 1 # xpd= optional; draw symols outside of plot area? TRUE or FALSE, default is FALSE","title":"Add a legend"},{"location":"#add-error-bars-to-points-example-for-standard-dev-sd-of-y-error-bars-this-is-a-quick-way-using-a-function-that-draws-arrows","text":"arrows(x0=, y0=, x1=, y1=, length=, angle=90, code=3, col=) # if x and y are the vectors you used to plot points(), then: # x0=, y0= vectors for coordinates FROM which to draw bars, e.g. x0=x, y0=y-SD # x1=, y1= vectors for coordinates TO which to draw bars, e.g. x1=x, y1=y+SD # length= length of the horizontal lines at the ends of the error bar, default is 0.25 # angle= angle of the horizontal lines at the ends of the error bar, leave at 90 # code= draw horizontal lines at the top, bottom or both sides of the error bar, leave at 3 # col= color of the error bars","title":"Add error bars to points (example for standard dev SD of y-error-bars) this is a quick way using a function that draws arrows"},{"location":"#add-other-lines-of-text-to-your-plot","text":"mtext(text=, side=, line=, font=) # text= string of text to print in the plot # side= at which side of the plot and in which orientation the text should be printed, side=1 bottom, =2 left, =3 top, =4 right # line= on which line the text should be printed, = 0 means outside of the plot, 0 means inside of the plot # font= style of the text; =1 plain, =2 bold, =3 italic, =4 bold italic # for superscript and subscript, text=expression() can be used; run the lines of code below to see an example plot(NULL, xlim=c(0,10), ylim=c(0,5), xaxs= i , yaxs= i , xaxt= n , yaxt= n , xlab= , ylab= , frame.plot=FALSE) mtext(text=expression( Normal text [ Subscript text ]* normal again ^ superscript * normal ), side=3, line=-3) mtext(text=expression( For numbers, this also works without quotation marks [420]*123^-2), side=3, line=-5) mtext(text=expression( This can also do that [ bottom ]^ top ), side=3, line=-7)","title":"Add other lines of text to your plot"},{"location":"#bar-chart","text":"barplot(height=, space=, names.arg=, beside=, horiz=, col=, border=, axes=, axisnames=) # height= vector or matrix of values for the height of the bars; in a matrix, the values in each column will be stacked into one bar # when using a dataframe called df as input, use as.matrix(df); you might also need to transpose the matrix with t(as.matrix(df)) # space= optional; distance between the bars; default is 0.2 # names.arg= optional; vector of strings for bar names to be drawn below x axis; default is NULL # beside= optional; TRUE or FALSE, if TRUE, values from columns are drawn next to each other and not stacked; default is FALSE # horiz= optional; TRUE or FALSE, if TRUE, bars are drawn horizontally; default is FALSE # col= optional; vector of colors for bars, row by row # border= optional; TRUE or FALSE, if TRUE, a box is drawn around each bar; default is TRUE # axes= optional; TRUE or FALSE, if TRUE, the y axis is drawn; default is TRUE # axisnames= optional; TRUE or FALSE, if TRUE, the names of the bars (see names.arg=) are drawn; default is TRUE Written by Martin Taubert Oct 2018","title":"Bar chart"},{"location":"part2/","text":"Data Manipulation and Selection with R Today we will practice slicing and dicing a dataframe to grab only the data we are interested in. I am not going to exhaustively cover all the features in R for searching and manipulating data and instead this will give you a snap shot in the most common methods I use. For a more complete discussion / further investigation I highly recommend you check out Hadley Wickham's post that can be found http://adv-r.had.co.nz/Subsetting.html . Today we'll be covering: Slicing a dataframe using row and column numbers Grabbing specific named columns or rows Identifying data based on operators (conditions) Using grep to search for specific keywords / regex Sorting data Doing all of the above with the amazing 'dplyr' package. Slicing a dataframe Specifying row and column numbers R has 5 basic data structures and we'll cover some of them today. I typically default to using data frames, and these are most similar to a two-dimensional table in excel. A matrix is a similar structure, but all the columns need to be the same type. Here I've named our data frame (df), however, you could pick any variable name you want, although try not to use special characters or already named functions. Data from a data frame can be accessed with the syntax dataframe[rows, columns] Hopefully the following examples will make this more clear. First import the data that we'll be using today. df - read.delim( simulated_dataset.txt , header=T) Try these examples: #Get the first 3 rows from the dataframe df[c(1,2,3),] #Get the first 3 columns from the dataframe df[,c(1,2,3)] #Get the first 3 rows and first 3 columns df[c(1,2,3),c(1,2,3)] #Similar to the bash command, head is a function that by default prints the first 6 rows head(df) head(df, 10) #Similarly, tail prints the last 6 rows tail(df) #You can chain the head function with specifying only some columns head(df[,c(1,2,3)]) #Use the minus sign to **remove** columns or rows df[-c(1,2,3),] df[,-c(1,2,3)] Now it is your turn, try to answer the following questions: . Grab the 1st, 3rd, and 5th rows of our data frame. . Get all data except the 3rd column. . Retrieve the last 6 rows. . Get the \"SampleID\", \"Well\", \"Month\", \"PH\", \"DO\", and \"DOC\" from all samples from the \"HTU\" aquifer and store the data in a new dataframe named \"df_HTU\". As a quick aside, we can use the rbind() function to combine two dataframes by rows. Try the following: df1 - df[c(1,2,3),] df2 - df[c(28, 29, 30),] #Use rbind to merge these 2 sliced dataframes df3 - rbind(df1, df2) #Or you can get the same result with these 2 commands: df3 - df[c(1,2,3,28,29,30),] df3 - df[c(seq(1,3), seq(28,30)),] Using ranges and formulas to slice a data frame Instead of specifying each row or column number, we can also generate these based on a range of numbers or by using functions in R that generate a vector of numbers For example: #Grab columns 3 through 5 df[,3:5] #Same outcome, but we can specify directly that the range is a vector df[,c(3:5)] #We can mix specifying numbers and generating a range df[c(1,2,10:15,19),] #Try df[1,2,10:15,19,] Once you get more familiar with some R functions that generate vectors we can also chain these together to specify rows or columns df[,seq(1,ncol(df), by=3)] . What is the seq command? . What is the ncol command? df[,seq(1,length(df), by=3)] . What is the difference between length and ncol here? . How would you select a range of rows that represent the last 5 rows in the dataframe? Answer: df[c((length(df)-5):length(df)),] And again, we can chain these together into a vector: df[c(1,2,seq(3,11, by=1)),] Try to fix these commands: . df[1,2,seq(3,11,2),] . Why do you not need to specify \"by\"? . df[c(1:5,seq(7,10)] Splicing a data frame by using column and/or row names Here we're using R conventions/shortcuts to pull out specific vectors based on their name. To start off with lets look at functions that view the names #Look at the column names colnames(df) #Look at the row names rownames(df) #What does this do? names(df) Lets try some examples: # Grab columns SampleID and PH df[,c( SampleID , PH )] # What is the difference in the following 2 commands? df[c(1,2,3),] df[c( 1 , 2 , 3 ),] Example Questions: . Get a dataframe with the SampleID and the Zone only. . Retrieve a dataframe with the sampleID and all continuous data columns. . Get a dataframe with Samples 1 through 6 with SampleID, Aquifer, TOC and TIC. Using Logical and Conditional Operators We can also select only rows or columns that match some condition we want. This can be something like all rows where the PH is less than 5, or all wells that match a specific value / string. But to start off with, lets go over some terms and syntax. We can refer to a column (vector) from a dataframe using the established shortcut dataframe-name$column-name. For example: df$SampleID df$PH Lets also quickly cover some of tools we can use to test our conditions Relational Operators (Guess what they mean) == != = = Logical Operators ! * | Only checks the first element || OK, probably pretty confusing, but lets go over some examples that should make these concepts more clear. df$PH 10 which(df$PH 10) #Whats the difference? df[df$PH 10, ] df[which(df$PH 10),] #What is R doing in the above commands? df$SampleID == Sample13 which(df$SampleID == Sample13 ) df[which(df$SampleID == Sample13 ),] df$PH 10 df$SampleID == Sample13 #Combining conditions df$PH 10 df$SampleID == Sample13 #example for ' ' vs ' ' df$PH 10 df$SampleID == Sample13 #checks to see if both boolean vectors are the same, returns single value df$PH 10 | df$SampleID == Sample13 #Think about what each of the following statements is doing. It can be helpful to translate these into English / German in your head or on paper. df[df$DO 0 df$NH4 0.1,] #For dataframe named df, return rows where the column DO values are greater than 0 AND where values from column NH4 are greater than 0.1; return all columns df[which(df$DO 0 df$NH4 0.1),] df[df$DO 0 df$NH4 0.2 df$Season == Summer ,] df[which(df$DO 0 df$NH4 0.2 df$Season == Summer ),] #What does this command return? Can you translate it? df[which(df$DO 0 df$NH4 0.2 | df$Season == Summer ),] #Some more complicated examples df[which(df$PH = 7.41 df$TOC 1.4 df$TOC 1.2),] df[which(df$PH 7.1 | df$PH 8.1),] Just as a comment, R is great because you can save a lot of the intermediate steps. If you find yourself getting lost in the statements, separate them out. Make sure that you reference the correct dataframe in each of the steps though! df_working - df df_working - df_working[df_working$PH = 7.41,] #MAKE SURE YOU USE THE CORRECT REFERENCE!!!! df_working - df_working[df_working$TOC 1.4,] df_working - df_working[df_working$TOC 1.2,] #Did this give you the same answer? How can you check? Using grep to find / search grep is a command-line program that search for lines that match a regular expression. We can use the R implementation of grep to search vectors for elements that match a regular expression / keyword. The syntax is grep(PATTERN, vector). Lets look at some examples: #Search for values containing HTU in the vector df$Zone df[grep( HTU , df$Zone),] #Here is an example of a simple regular expression, it reads search for HTU4 OR HTU5 within the vector df$Zone df[grep( HTU4|HTU5 , df$Zone),] #A more complicated regex, search for a string that ends with a two-digit number within the SampleID df[grep( .*[0-9]{2} , df$SampleID),] #The .* means match anything. #What is this one doing? df[c(grep( .*er , df$Season), grep( Autumn , df$Season)),] A word of caution. I often find myself doing stupid stuff like: df[grep( Sample2[0-9] ,df$SampleID),][df[grep( Sample2[0-9] ,df$SampleID),]$DO 0.1,] Can you figure out what is going on? It is messy, ugly, and hard to read. Seeing code like this in your Rscripts is always sad since it will take you much longer to figure out what you were doing, and it will likely obscure bugs in your code. It would be better to split your commands up: df_sample20_up - df[grep( Sample2[0-9] ,df$SampleID),] df_sample20_up[df_sample20_up$DO 0.1,] . Get all data that was collected during June and July. Sorting data in R Confusingly, you will almost always use the order() command to sort your data in R. Order gives you a vector with indices of the sorted data, while sort gives you the actual sorted vector. For example: sort(df$SampleID) order(df$SampleID) In general, we use the vector that order returns to sort the full dataframe. So we can do stuff like this: #Sort the dataframe based on the values in column SampleID df[order(df$SampleID),] #What happened here? #Sort using a numeric value df[order(row.names(df), decreasing = T),] df[order(as.numeric(row.names(df)), decreasing = T),] #If we didn't have a numeric column to sort on we can split apart a column to sort on df[order(as.numeric(gsub( Sample , , df$SampleID)), decreasing = T),] #Here I am using gsub, the find replace version of grep, to replace Sample with (nothing), then I'm telling R that these are numbers not characters, finally I tell it to order these numbers from highest to lowest. #Why does R sort these numerically by default? df[order(df$PH),] #You can check the type of vector you have with: str(df$PH) #Or to check all vectors in a dataframe you can use str(df) Sometimes you want to sort using multiple columns. Order allows you to specify multiple columns and it will sort your data in that order. df[order(df$PH, df$DO),] df[order(df$PH, -df$DO),] This question came up when I was working on this dataset and I think it is a fairly typical question. How can we order our dataset based on Months? Lets check what we are starting with: df$Month It seems like all are the common 3 letter abbreviations, except July (There are ALWAYS exceptions in your data) Lets fix that: df[df$Month == July ,]$Month - Jul #Gives error, missing values are not allowed OK, that didn't work. What happened? Unfortunately it messed up the whole dataframe by adding NAs. Lets reload the data first. df - read.delim(file = simulated_dataset.txt , header=T) So after googling, I relearned that changing the values of \"factors\" to a new value doesn't work well. Lets change months to a \"character\" data type. df$Month - as.character(df$Month) Now we can try to make the substitution: df[df$Month == July ,]$Month - Jul Success!! Now we need to tell R how we want the data organized. I couldn't think of a good way to order Months alphabetically in a manner that works. But luckily R has a defined vector of the common 3 letter abbreviations, and it is already sorted. We can use this defined order, to structure our df$Month column. Here I am telling R to treat Months as a factor again, and then defining the order of the values of that factor. I know this is confusing, I typically google how to do this every time. It is extremely useful though, since many of the graphing packages order your data based on the factor levels. df$Month - factor(df$Month, levels = month.abb) Now we can order the full dataframe based on our Month factor: df[order(df$Month, decreasing = F),] Yes, I still think it is MAGIC that this works. Seriously. Summary Questions List the names for each of the data columns in df. What is the name of the 3rd column in df. Get the 9-12 rows and 4-10 columns from df. Which samples from well H41 have TOC values less than 0.8? Make a new dataframe that only contains samples collected from the \"HTL\" aquifer. Using this new dataframe, which samples had a higher temperature than 4.5? Make another new dataframe that consists of 10 samples, 5 with the lowest NO3 and 5 with the highest NO3 measurements. Re-order the original dataframe by PO4, with the lowest value at the top. Re-order the original dataframe by PO4 and Mg, with the highest values at the top. Re-order the original dataframe by PO4 with the highest values at the top, and then by Mg with the lowest values at the top. Do the same as above, but reverse the order (Mg first, then PO4). Rename the samples \"AquaDivaN\" when N is 1 to 30. Make 4 new dataframes, 1 for each season. Any other ways of manipulations data that you want / need? Manipulating and Summarizing our data with dplyr dplyr is a very helpful package that is specifically designed to work on dataframes and work QUICKLY . It is the updated version of plyr. I recommend checking out the following link for some ways to user dplyr. https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html We'll go through the basic syntax dplyr uses and some examples. library(dplyr) #reload the dataframe if necessary df - read.delim( simulated_dataset.txt , header=T) Filter the dataframe Here we can grab the rows that match criteria we set. This is similar to the conditional operators we used in the above section. Note: the % % symbol can be read as a \"pipe\" where we are chaining together a series of commands. df % % filter(Season == Summer ) df % % filter(Season == Summer DO 0.4) df % % filter(Well != H41 PH 7.4 DOC 2) filter(df, Season == Summer ) Sort the dataframe We can use the arrange command in dplyr to sort our dataframe. This may be easier to remember than order. df % % arrange(desc(DO), desc(NH4), Month) Select columns from the dataframe df % % select(SampleID, DO, PH, Mg, Season) df % % select(SampleID, TEMP_W_ES:Na) df % % select(-(Well:Aquifer)) df % % select(SampleID, starts_with( EC )) Add new columns based on functions of previous columns Many times you will import a relatively simple dataset into R and then want to perform several calculations using that data. For instance you want to convert date / time column to \"Hours since start\", or you want to convert you absorbance data to counts based on a standard curve, etc... Here we can use dplyr to run those calculations and add the result to a new column. This example will convert our DO measurements to percent oxygen saturation. df - df % % mutate(TEMP_K = TEMP_W_ES + 273.15) #DO Saturation Formula assuming salinty is 0: # exp(-173.4292 + 249.6339*100/T + 143.3483*ln(T/100) + - 21.8492 * (T/100)) df - df % % mutate(DO_sat = exp(-173.4292 + (249.6339*100/TEMP_K) + (143.3483*log(TEMP_K/100)) + (-21.8492 * (TEMP_K/100)))) df - df % % mutate(O2_perc = DO / DO_sat * 100) df % % select(SampleID:Aquifer, O2_perc) There is also a function in case we only care about the results and don't want to add it to the exisiting dataframe. #Save only new column df % % transmute(O2_perc = DO / DO_sat*100) Summarise / Summarize This is usually how I end up using dplyr. It is an incredibly powerful way of quickly calculating summary statistics on your data while you try and explore what is going on. This would be analagous to using pivot tables in excel. I find the dplyr syntax really nice for reading exactly what you are doing. For example, we can calculate per Well statistics for a specific variable of interest. df % % group_by(Well) % % summarize(mean_O2_perc = mean(O2_perc), sd_o2_perc = sd(O2_perc)) #Take the dataframe df, split into smaller dataframes on the fly based on the values found in the df$Well column. Run the summarise() function on each of the these smaller dataframes to calculate the mean and standard deviation for O2 percent. We can do something similar and calculate season based statistics: df % % group_by(Season) % % summarize(n = n(), mean_PH=mean(PH), mean_TOC=mean(TOC)) #Split dataframe df into smaller dataframes based on the values in the column Season , the summarize these smaller dataframes by calculating the number of samples (n), and the mean for PH and TOC Sometimes its useful to split the dataframes based on multiple values. Here we will generate summary statistics for each Well based on the sampling season. df % % group_by(Well, Season) % % summarize(n=n(), mean=mean(O2_perc)) df % % group_by(Well, Season) % % summarize(n=n(), mean=mean(O2_perc)) % % arrange(mean, Well) There are also some nice functions were we can generate summary statistics for all data columns at once, such as: df % % select(Well, Season, TEMP_W_ES:Na) % % group_by(Well, Season) % % summarize_all(funs(mean)) df_sum - df % % group_by(Well, Season) % % summarize_at(.vars = vars(TEMP_W_ES:Na), .funs = c(n= length , mean= mean , sd= sd )) #I haven't figured out the best way to order these columns yet, but this is a quick and dirty way. df_sum % % select(Well, Season, order(names(df_sum))) dplyr Summary Questions Questions 15-18: re-do questions 4, 5, 8, and 10 using dplyr functions . Find the mean SO4 levels for the two aquifers . Find the mean SO4 levels for the two aquifers each season . Add a new column with the Hydronium ion concentration for each sample (hint: H3O+ = 10^(-pH) ) . Add a new column with the pOH value for each sample (pH + pOH = 14) Written by Will A. Overholt Oct 2018","title":"Data manipulation"},{"location":"part2/#data-manipulation-and-selection-with-r","text":"Today we will practice slicing and dicing a dataframe to grab only the data we are interested in. I am not going to exhaustively cover all the features in R for searching and manipulating data and instead this will give you a snap shot in the most common methods I use. For a more complete discussion / further investigation I highly recommend you check out Hadley Wickham's post that can be found http://adv-r.had.co.nz/Subsetting.html . Today we'll be covering: Slicing a dataframe using row and column numbers Grabbing specific named columns or rows Identifying data based on operators (conditions) Using grep to search for specific keywords / regex Sorting data Doing all of the above with the amazing 'dplyr' package.","title":"Data Manipulation and Selection with R"},{"location":"part2/#slicing-a-dataframe","text":"","title":"Slicing a dataframe"},{"location":"part2/#specifying-row-and-column-numbers","text":"R has 5 basic data structures and we'll cover some of them today. I typically default to using data frames, and these are most similar to a two-dimensional table in excel. A matrix is a similar structure, but all the columns need to be the same type. Here I've named our data frame (df), however, you could pick any variable name you want, although try not to use special characters or already named functions. Data from a data frame can be accessed with the syntax dataframe[rows, columns] Hopefully the following examples will make this more clear. First import the data that we'll be using today. df - read.delim( simulated_dataset.txt , header=T) Try these examples: #Get the first 3 rows from the dataframe df[c(1,2,3),] #Get the first 3 columns from the dataframe df[,c(1,2,3)] #Get the first 3 rows and first 3 columns df[c(1,2,3),c(1,2,3)] #Similar to the bash command, head is a function that by default prints the first 6 rows head(df) head(df, 10) #Similarly, tail prints the last 6 rows tail(df) #You can chain the head function with specifying only some columns head(df[,c(1,2,3)]) #Use the minus sign to **remove** columns or rows df[-c(1,2,3),] df[,-c(1,2,3)] Now it is your turn, try to answer the following questions: . Grab the 1st, 3rd, and 5th rows of our data frame. . Get all data except the 3rd column. . Retrieve the last 6 rows. . Get the \"SampleID\", \"Well\", \"Month\", \"PH\", \"DO\", and \"DOC\" from all samples from the \"HTU\" aquifer and store the data in a new dataframe named \"df_HTU\". As a quick aside, we can use the rbind() function to combine two dataframes by rows. Try the following: df1 - df[c(1,2,3),] df2 - df[c(28, 29, 30),] #Use rbind to merge these 2 sliced dataframes df3 - rbind(df1, df2) #Or you can get the same result with these 2 commands: df3 - df[c(1,2,3,28,29,30),] df3 - df[c(seq(1,3), seq(28,30)),]","title":"Specifying row and column numbers"},{"location":"part2/#using-ranges-and-formulas-to-slice-a-data-frame","text":"Instead of specifying each row or column number, we can also generate these based on a range of numbers or by using functions in R that generate a vector of numbers For example: #Grab columns 3 through 5 df[,3:5] #Same outcome, but we can specify directly that the range is a vector df[,c(3:5)] #We can mix specifying numbers and generating a range df[c(1,2,10:15,19),] #Try df[1,2,10:15,19,] Once you get more familiar with some R functions that generate vectors we can also chain these together to specify rows or columns df[,seq(1,ncol(df), by=3)] . What is the seq command? . What is the ncol command? df[,seq(1,length(df), by=3)] . What is the difference between length and ncol here? . How would you select a range of rows that represent the last 5 rows in the dataframe? Answer: df[c((length(df)-5):length(df)),] And again, we can chain these together into a vector: df[c(1,2,seq(3,11, by=1)),] Try to fix these commands: . df[1,2,seq(3,11,2),] . Why do you not need to specify \"by\"? . df[c(1:5,seq(7,10)]","title":"Using ranges and formulas to slice a data frame"},{"location":"part2/#splicing-a-data-frame-by-using-column-andor-row-names","text":"Here we're using R conventions/shortcuts to pull out specific vectors based on their name. To start off with lets look at functions that view the names #Look at the column names colnames(df) #Look at the row names rownames(df) #What does this do? names(df) Lets try some examples: # Grab columns SampleID and PH df[,c( SampleID , PH )] # What is the difference in the following 2 commands? df[c(1,2,3),] df[c( 1 , 2 , 3 ),] Example Questions: . Get a dataframe with the SampleID and the Zone only. . Retrieve a dataframe with the sampleID and all continuous data columns. . Get a dataframe with Samples 1 through 6 with SampleID, Aquifer, TOC and TIC.","title":"Splicing a data frame by using column and/or row names"},{"location":"part2/#using-logical-and-conditional-operators","text":"We can also select only rows or columns that match some condition we want. This can be something like all rows where the PH is less than 5, or all wells that match a specific value / string. But to start off with, lets go over some terms and syntax. We can refer to a column (vector) from a dataframe using the established shortcut dataframe-name$column-name. For example: df$SampleID df$PH Lets also quickly cover some of tools we can use to test our conditions Relational Operators (Guess what they mean) == != = = Logical Operators ! * | Only checks the first element || OK, probably pretty confusing, but lets go over some examples that should make these concepts more clear. df$PH 10 which(df$PH 10) #Whats the difference? df[df$PH 10, ] df[which(df$PH 10),] #What is R doing in the above commands? df$SampleID == Sample13 which(df$SampleID == Sample13 ) df[which(df$SampleID == Sample13 ),] df$PH 10 df$SampleID == Sample13 #Combining conditions df$PH 10 df$SampleID == Sample13 #example for ' ' vs ' ' df$PH 10 df$SampleID == Sample13 #checks to see if both boolean vectors are the same, returns single value df$PH 10 | df$SampleID == Sample13 #Think about what each of the following statements is doing. It can be helpful to translate these into English / German in your head or on paper. df[df$DO 0 df$NH4 0.1,] #For dataframe named df, return rows where the column DO values are greater than 0 AND where values from column NH4 are greater than 0.1; return all columns df[which(df$DO 0 df$NH4 0.1),] df[df$DO 0 df$NH4 0.2 df$Season == Summer ,] df[which(df$DO 0 df$NH4 0.2 df$Season == Summer ),] #What does this command return? Can you translate it? df[which(df$DO 0 df$NH4 0.2 | df$Season == Summer ),] #Some more complicated examples df[which(df$PH = 7.41 df$TOC 1.4 df$TOC 1.2),] df[which(df$PH 7.1 | df$PH 8.1),] Just as a comment, R is great because you can save a lot of the intermediate steps. If you find yourself getting lost in the statements, separate them out. Make sure that you reference the correct dataframe in each of the steps though! df_working - df df_working - df_working[df_working$PH = 7.41,] #MAKE SURE YOU USE THE CORRECT REFERENCE!!!! df_working - df_working[df_working$TOC 1.4,] df_working - df_working[df_working$TOC 1.2,] #Did this give you the same answer? How can you check?","title":"Using Logical and Conditional Operators"},{"location":"part2/#using-grep-to-find-search","text":"grep is a command-line program that search for lines that match a regular expression. We can use the R implementation of grep to search vectors for elements that match a regular expression / keyword. The syntax is grep(PATTERN, vector). Lets look at some examples: #Search for values containing HTU in the vector df$Zone df[grep( HTU , df$Zone),] #Here is an example of a simple regular expression, it reads search for HTU4 OR HTU5 within the vector df$Zone df[grep( HTU4|HTU5 , df$Zone),] #A more complicated regex, search for a string that ends with a two-digit number within the SampleID df[grep( .*[0-9]{2} , df$SampleID),] #The .* means match anything. #What is this one doing? df[c(grep( .*er , df$Season), grep( Autumn , df$Season)),] A word of caution. I often find myself doing stupid stuff like: df[grep( Sample2[0-9] ,df$SampleID),][df[grep( Sample2[0-9] ,df$SampleID),]$DO 0.1,] Can you figure out what is going on? It is messy, ugly, and hard to read. Seeing code like this in your Rscripts is always sad since it will take you much longer to figure out what you were doing, and it will likely obscure bugs in your code. It would be better to split your commands up: df_sample20_up - df[grep( Sample2[0-9] ,df$SampleID),] df_sample20_up[df_sample20_up$DO 0.1,] . Get all data that was collected during June and July.","title":"Using grep to find / search"},{"location":"part2/#sorting-data-in-r","text":"Confusingly, you will almost always use the order() command to sort your data in R. Order gives you a vector with indices of the sorted data, while sort gives you the actual sorted vector. For example: sort(df$SampleID) order(df$SampleID) In general, we use the vector that order returns to sort the full dataframe. So we can do stuff like this: #Sort the dataframe based on the values in column SampleID df[order(df$SampleID),] #What happened here? #Sort using a numeric value df[order(row.names(df), decreasing = T),] df[order(as.numeric(row.names(df)), decreasing = T),] #If we didn't have a numeric column to sort on we can split apart a column to sort on df[order(as.numeric(gsub( Sample , , df$SampleID)), decreasing = T),] #Here I am using gsub, the find replace version of grep, to replace Sample with (nothing), then I'm telling R that these are numbers not characters, finally I tell it to order these numbers from highest to lowest. #Why does R sort these numerically by default? df[order(df$PH),] #You can check the type of vector you have with: str(df$PH) #Or to check all vectors in a dataframe you can use str(df) Sometimes you want to sort using multiple columns. Order allows you to specify multiple columns and it will sort your data in that order. df[order(df$PH, df$DO),] df[order(df$PH, -df$DO),] This question came up when I was working on this dataset and I think it is a fairly typical question. How can we order our dataset based on Months? Lets check what we are starting with: df$Month It seems like all are the common 3 letter abbreviations, except July (There are ALWAYS exceptions in your data) Lets fix that: df[df$Month == July ,]$Month - Jul #Gives error, missing values are not allowed OK, that didn't work. What happened? Unfortunately it messed up the whole dataframe by adding NAs. Lets reload the data first. df - read.delim(file = simulated_dataset.txt , header=T) So after googling, I relearned that changing the values of \"factors\" to a new value doesn't work well. Lets change months to a \"character\" data type. df$Month - as.character(df$Month) Now we can try to make the substitution: df[df$Month == July ,]$Month - Jul Success!! Now we need to tell R how we want the data organized. I couldn't think of a good way to order Months alphabetically in a manner that works. But luckily R has a defined vector of the common 3 letter abbreviations, and it is already sorted. We can use this defined order, to structure our df$Month column. Here I am telling R to treat Months as a factor again, and then defining the order of the values of that factor. I know this is confusing, I typically google how to do this every time. It is extremely useful though, since many of the graphing packages order your data based on the factor levels. df$Month - factor(df$Month, levels = month.abb) Now we can order the full dataframe based on our Month factor: df[order(df$Month, decreasing = F),] Yes, I still think it is MAGIC that this works. Seriously.","title":"Sorting data in R"},{"location":"part2/#summary-questions","text":"List the names for each of the data columns in df. What is the name of the 3rd column in df. Get the 9-12 rows and 4-10 columns from df. Which samples from well H41 have TOC values less than 0.8? Make a new dataframe that only contains samples collected from the \"HTL\" aquifer. Using this new dataframe, which samples had a higher temperature than 4.5? Make another new dataframe that consists of 10 samples, 5 with the lowest NO3 and 5 with the highest NO3 measurements. Re-order the original dataframe by PO4, with the lowest value at the top. Re-order the original dataframe by PO4 and Mg, with the highest values at the top. Re-order the original dataframe by PO4 with the highest values at the top, and then by Mg with the lowest values at the top. Do the same as above, but reverse the order (Mg first, then PO4). Rename the samples \"AquaDivaN\" when N is 1 to 30. Make 4 new dataframes, 1 for each season. Any other ways of manipulations data that you want / need?","title":"Summary Questions"},{"location":"part2/#manipulating-and-summarizing-our-data-with-dplyr","text":"dplyr is a very helpful package that is specifically designed to work on dataframes and work QUICKLY . It is the updated version of plyr. I recommend checking out the following link for some ways to user dplyr. https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html We'll go through the basic syntax dplyr uses and some examples. library(dplyr) #reload the dataframe if necessary df - read.delim( simulated_dataset.txt , header=T)","title":"Manipulating and Summarizing our data with dplyr"},{"location":"part2/#filter-the-dataframe","text":"Here we can grab the rows that match criteria we set. This is similar to the conditional operators we used in the above section. Note: the % % symbol can be read as a \"pipe\" where we are chaining together a series of commands. df % % filter(Season == Summer ) df % % filter(Season == Summer DO 0.4) df % % filter(Well != H41 PH 7.4 DOC 2) filter(df, Season == Summer )","title":"Filter the dataframe"},{"location":"part2/#sort-the-dataframe","text":"We can use the arrange command in dplyr to sort our dataframe. This may be easier to remember than order. df % % arrange(desc(DO), desc(NH4), Month)","title":"Sort the dataframe"},{"location":"part2/#select-columns-from-the-dataframe","text":"df % % select(SampleID, DO, PH, Mg, Season) df % % select(SampleID, TEMP_W_ES:Na) df % % select(-(Well:Aquifer)) df % % select(SampleID, starts_with( EC ))","title":"Select columns from the dataframe"},{"location":"part2/#add-new-columns-based-on-functions-of-previous-columns","text":"Many times you will import a relatively simple dataset into R and then want to perform several calculations using that data. For instance you want to convert date / time column to \"Hours since start\", or you want to convert you absorbance data to counts based on a standard curve, etc... Here we can use dplyr to run those calculations and add the result to a new column. This example will convert our DO measurements to percent oxygen saturation. df - df % % mutate(TEMP_K = TEMP_W_ES + 273.15) #DO Saturation Formula assuming salinty is 0: # exp(-173.4292 + 249.6339*100/T + 143.3483*ln(T/100) + - 21.8492 * (T/100)) df - df % % mutate(DO_sat = exp(-173.4292 + (249.6339*100/TEMP_K) + (143.3483*log(TEMP_K/100)) + (-21.8492 * (TEMP_K/100)))) df - df % % mutate(O2_perc = DO / DO_sat * 100) df % % select(SampleID:Aquifer, O2_perc) There is also a function in case we only care about the results and don't want to add it to the exisiting dataframe. #Save only new column df % % transmute(O2_perc = DO / DO_sat*100)","title":"Add new columns based on functions of previous columns"},{"location":"part2/#summarise-summarize","text":"This is usually how I end up using dplyr. It is an incredibly powerful way of quickly calculating summary statistics on your data while you try and explore what is going on. This would be analagous to using pivot tables in excel. I find the dplyr syntax really nice for reading exactly what you are doing. For example, we can calculate per Well statistics for a specific variable of interest. df % % group_by(Well) % % summarize(mean_O2_perc = mean(O2_perc), sd_o2_perc = sd(O2_perc)) #Take the dataframe df, split into smaller dataframes on the fly based on the values found in the df$Well column. Run the summarise() function on each of the these smaller dataframes to calculate the mean and standard deviation for O2 percent. We can do something similar and calculate season based statistics: df % % group_by(Season) % % summarize(n = n(), mean_PH=mean(PH), mean_TOC=mean(TOC)) #Split dataframe df into smaller dataframes based on the values in the column Season , the summarize these smaller dataframes by calculating the number of samples (n), and the mean for PH and TOC Sometimes its useful to split the dataframes based on multiple values. Here we will generate summary statistics for each Well based on the sampling season. df % % group_by(Well, Season) % % summarize(n=n(), mean=mean(O2_perc)) df % % group_by(Well, Season) % % summarize(n=n(), mean=mean(O2_perc)) % % arrange(mean, Well) There are also some nice functions were we can generate summary statistics for all data columns at once, such as: df % % select(Well, Season, TEMP_W_ES:Na) % % group_by(Well, Season) % % summarize_all(funs(mean)) df_sum - df % % group_by(Well, Season) % % summarize_at(.vars = vars(TEMP_W_ES:Na), .funs = c(n= length , mean= mean , sd= sd )) #I haven't figured out the best way to order these columns yet, but this is a quick and dirty way. df_sum % % select(Well, Season, order(names(df_sum)))","title":"Summarise / Summarize"},{"location":"part2/#dplyr-summary-questions","text":"Questions 15-18: re-do questions 4, 5, 8, and 10 using dplyr functions . Find the mean SO4 levels for the two aquifers . Find the mean SO4 levels for the two aquifers each season . Add a new column with the Hydronium ion concentration for each sample (hint: H3O+ = 10^(-pH) ) . Add a new column with the pOH value for each sample (pH + pOH = 14) Written by Will A. Overholt Oct 2018","title":"dplyr Summary Questions"},{"location":"part3/","text":"Data visualization and exploration Synopsis In this part of the workshop you will familiarize yourself with: R's basic plotting capabilities ggplot2 , its syntax and fundamentals as well as some advanced data visualization If you have questions ASK, feel free to drop me an e-mail also after the workshop. Setting the stage Plotting is a very personal thing (btw color schemes as well), ask three different people and you will get a variety of feedback with respect to plotting and data visualization in general. A lot of people get away with Excel , SigmaPlot , GraphPad , Matplotlib (if you are a Pythonista, which makes you a good person by default) or something else. R as its parent S is a programming language primarily dedicated to data manipulation (in a good sense) and statistical analysis. Due to its modular architecture and the availabilitry of sophisticated libraries for data processing and plotting it is an excellent choice for any kind of data visualization. When I got in touch with R the first time, I was googling options how to do a complex multi-panel figure. The original query was \"plot facets vector graphics\". If you do the exact same query now, you will end up with ggplot2 being the second hit. Yep, this is how I \"met\" R and years later I would still lie if I would say that I'm any sort of R expert. Most of me using R is still centered around the following dogma: Define the problem Look up necessary R resources Apply available examples to own data Be happy after A LOT of try and error. Ok, first things first, fire up RStudio and check whether you have the following packages installed and load them. Throughout the course you can either execute commands from within a script (mark the respective lines of code and hit CTRL+ENTER) or directly in the console. library( ggplot2 ) library( dplyer ) library( tidyr ) library( ellipse ) library( RColorBrewer ) Ideally you should see no error message popping up, if you see any then you did not prepare properly for the workshop - shame on you, or rather me, because I did not tell you in time. Basic plotting Enough blabla, this session is about data visualization, where are the plots? Here they come. Create a new R script, type the following two lines and execute them: dotchart(rnorm(250), col = blue , main = Quick ugly example ) hist(rnorm(250), col = blue , main = Quick ugly example II ) Alternatively type them one after another in the console. Keep an eye on the plot window, what did just happen? Question Try to figure out what the individual functions and parameters do. First take home message of the day - the R help is your friend (a good one). The general syntax for calling R's help is: ?functionXYZ() Let's start exploring R's basic plotting capabilities in a bit more detail, this is a bit of a recap of what you did yesterday. # Let's define two arbitrary vectors bacteria - c(10, 30, 60, 5, 90) archaea - c(25, 27, 22, 37, 10) # Plot them both plot(bacteria, type= o , col = orange ) lines(archaea, type= o , pch=22, lty=2, col = blue ) axis(1, at=1:5, lab=(c( March , April , May , June , July ))) Let's say what we just plotted are relative abundances for archaea and bacteria across different months. But the output looks like garbage, apparently we had at first default x-axis labels, which were overwritten. The result is this \"beauty\" of a plot. How to fix that, take a look at the following code: plot(bacteria, type= o , col = orange , axes=FALSE, ann = FALSE) lines(archaea, type= o , pch=22, lty=2, col = blue ) axis(1, at=1:5, lab=(c( March , April , May , June , July ))) Does that make it any better? What is now missing? Exercise Call the help for axis(), box(), title(), and legend. Add a y-axis, a box around the plot, titles for the plot as well as the two axes, and a legend. One possible solution: plot(bacteria, type= o , col = orange , axes=FALSE, xlab = Month , ylab = Rel. abundance [%] , main = Bac and Arc ) lines(archaea, type= o , pch=22, lty=3, col = blue ) axis(1, at=1:5, lab=(c( March , April , May , June , July ))) box() axis(2, las = 2) legend(1, max(bacteria), c( Bacteria , Archaea ), cex=0.8, col=c( orange , blue ), pch=c(21,22), lty=c(1,3)) Alright, so far we played with a dataset that we quickly created, as you already learned before you can easily import datasets like this from any delimited file. Imagine a file like this (e.g. table.tsv): bacteria archaea 10 25 30 27 60 22 5 37 90 10 Two columns, tab-delimited, and the columns have names (\"bacteria2, \"archaea\"). We could read this file as outlined below. # Read the table, pay attention to the header and sep arguments rel_prok - read.table( table.tsv , header=T, sep= \\t ) # Instead we merge our two vectors because you did a lot of importing yesterday... rel_prok - data.frame(bacteria, archaea) colnames(rel_prok) - c(bacteria, archaea) # We define colors to be used with our data series, because why not plot_colors - c( blue , orange ) # We initiate a PNG devide to save the output png(filename= output.png , height=250, width=300, bg= white ) # AND NOW?! # Adapt your code # and end it with dev.off() # to turn off the PNG device Exercise Plot the data as before, and save the output as a .png. Do you have to adjust the dimensions? What is the dev.off() function doing? Base R data visualization options So far we basically only did line charts, base R provides us however with a whole range of different visualization options. Let's take one of our vectors and see how we can create bar charts and how we can visualize both data series by dot charts. Bar charts: bacteria - c(10, 30, 60, 5, 90) # A simple bar plot barplot(bacteria, main= Bacteria relative abundance , xlab= Month , ylab= Rel. abundance [%] , names.arg=c( March , April , May , June , July )) # Simple, and slightly pimped, pattern fill barplot(bacteria, main= Bacteria relative abundance , xlab= Month , ylab= Rel. abundance [%] , names.arg=c( March , April , May , June , July ), border= gray , density=c(10,20,30,40,50)) # Add a box around the plot because we like boxes box() # This time with colors barplot(bacteria, main= Bacteria relative abundance , xlab= Month , ylab= Rel. abundance [%] , names.arg=c( March , April , May , June , July ), col=rainbow(5)) And a dot chart: # Plot the dotchart dotchart(t(rel_prok), color=c( blue , red ), main= Dotchart Bacteria and Archaea ) We finish this first session with a little exercise. Exercise Use the simple dataset to plot a grouped bar chart incl. a legend and a dot chart with months as row names. Export both as .png files. NOT using RStudio's export function. For more examples of using the plotting capabilities of base R have a look for instance here . Solutions: # Grouped bar chart barplot(as.matrix(rel_prok), main= Bac vs Arc , ylab= Rel. abundance , beside=TRUE, col=rainbow(5)) box() # Place the legend at the top-left corner with no frame # using rainbow colors legend( topleft , c( March , April , May , June , July ), cex=1, bty= n , fill=rainbow(5)) # Dot chart with months as labels row.names(rel_prok) - c( March , April , May , June , July ) rel_prok dotchart(t(rel_prok), color=c( blue , red ), main= Dotchart Bacteria and Archaea , cex = 1) Basic usage of ggplot2 ggplot2 Base R's plotting capabilities are not bad, but the bottom line is, even with a lot of tweaking the resulting plots are visually rather less appealing. When you are dealing with your data, you want to present it in the best possible/convincing way. Often when I read papers and I see mediocre figures I think one thing, namely: \"RESPECT YOUR DATA!\". Investing time in in proper plotting/visualization/beautifying is usually more than worth it as it pays off in multiple regards. Luckily, there are a multitude of R packages that provide us with almost unlimited options of data visualization. The most common one is ggplot2 , which was created and is maintained by Hadley Wickham . Hadley is incredibly active in the R scene and maintains a lot of popular R packages/tools including: dplyr tidyr stringr ggplot2 to name a few. He is also one of the main persons behind RStudio . ggplot2 versus base R Plotting in base R can be mostly done using data stored in vectors. In comparison, ggplot2 relies on dataframes. Let's have a look at one of these as a quick reminder: data(mtcars) head(mtcars) mpg cyl disp hp drat wt qsec vs am gear carb Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Dataframes are nothing else but lists of vectors of equal lengths. It we take a qick look at our workshop mock data, are we dealing with a dataframe as well? aqd_mock - read.table( simulated_dataset.txt , header=T, sep= \\t , row.names = SampleID ) head(aqd_mock) Well Month Season Cluster Zone Aquifer TEMP_W_ES EC EC25 PH DO NH4 PO4 DOC TOC TIC NO3 SO4 Cl Ca Sample1 H41 Mar Spring 2 HTL HTL 4.4 455 113.7800 7.41 6.41 0.0 0.4 1.3 1.3 36.81 24.34 21.86 5.6 34.7 Sample2 H41 Jun Summer 2 HTL HTL 4.5 465 113.2046 8.21 5.50 0.1 0.1 1.1 1.2 37.45 19.29 15.33 5.8 33.7 Sample3 H41 Aug Summer 2 HTL HTL 4.7 444 108.1701 7.31 3.71 0.2 0.9 1.7 1.8 32.23 12.98 19.24 5.6 30.9 Sample4 H41 Nov Autumn 2 HTL HTL 4.8 449 110.1839 7.31 3.20 0.2 0.1 1.5 1.7 35.84 12.04 16.49 5.7 30.3 Sample5 H41 Feb Winter 2 HTL HTL 3.9 457 115.9376 7.21 6.99 0.0 1.1 0.7 0.8 34.27 23.79 26.22 5.9 35.4 Sample6 H41 May Spring 2 HTL HTL 4.8 477 116.3691 7.31 7.83 0.0 2.4 0.7 0.7 35.37 26.71 23.91 5.6 36.0 Fe Mg Mn Na Sample1 0.0 23.17 0.00 3.0 Sample2 0.0 24.51 0.00 2.0 Sample3 5.2 29.54 0.35 3.5 Sample4 1.4 29.00 0.34 3.3 Sample5 6.0 23.90 0.09 3.5 Sample6 2.7 21.95 0.09 1.6 Apparently we do. BTW: Question Why do we add the \"header\" and \"row.names\" parameters? Besides from the usage of dataframes, the second key characteristics of ggplot2 is that you work with layers. Basically, every ggplot2 object is like a canvas and we keep painting on it by adding layers, aka geoms. library( ggplot2 ) # Initialize a ggplot object ggplot(aqd_mock, aes(x=Zone, y=Fe)) Thats our canvas, and yes so far it is fairly empty. We have to fill it by adding the aforementioned geom objects, before we take a look at the overall ggplot2 syntax. ggplot( data = some_data_frame , mapping = aes( x = some_column , y = some_other_column , random aesthetics = based_on_a_random_parameter ) ) + geom_ some_plot_type () Now we add some content. ggplot(aqd_mock, aes(x=Zone, y=Fe)) + geom_point() We just created our first ggplot2 plot. Whoa. Basic customizations / a bit about aesthetics Lets be more serios, our dataset spans data from different seasons. One obvious question is whether we can identify differences over the year? Let's find out and color our dots according to season. # A touch of color ggplot(aqd_mock, aes(x=Well, y=Fe, col = Season)) + geom_point() We can use this simple example to learn more about how ggplot2 aesthetics work. ggplot(aqd_mock, aes(x=Zone, y=Fe)) + geom_point(aes(col = Season)) ggplot(aqd_mock, aes(x=Zone, y=Fe, col = Season)) + geom_point(colour = Black ) # Why does the following not work? ggplot(aqd_mock, aes(x=Zone, y=Fe, col = Season)) + geom_point(aes(col = Black )) # A second variable to modify aesthetics ggplot(aqd_mock, aes(x=Zone, y=Fe)) + geom_point(aes(col = Season, shape = Well)) # That does not make so much sense hm? ggplot(aqd_mock, aes(x=Zone, y=Fe)) + geom_point(aes(col = Season, shape = Aquifer)) ggplot(aqd_mock, aes(x=Zone, y=Fe)) + geom_point(aes(fill = Season, shape = factor(Aquifer), alpha = .6, size = 5), colour = Black ) + scale_shape_manual(values=c(21,22)) # Getting rid of some legends ggplot(aqd_mock, aes(x=Zone, y=Fe)) + geom_point(aes(fill = Season, shape = factor(Aquifer), alpha = .6, size = 5), colour = Black ) + scale_shape_manual(values=c(21,22)) + guides(size = FALSE, alpha = FALSE) # Success Ah well, so far nothing really obvious, however right now it is really hard to tell. Let's try to visualize this better. Exercise Check out geom_boxplot() via ?geom_boxplot() Plot box plots combined with dot plots for the Fe content at the different wells, using the season as grouping. Try out geom_violin() as well. Does that that help, do we see differences across season now? # A first boxplot ggplot(aqd_mock, aes(x=Well, y=Fe)) + geom_boxplot(alpha = .6) + geom_point(aes(fill=Season ,size = 5, shape = factor(Aquifer), alpha = .6), colour = Black , position = position_jitterdodge()) + scale_shape_manual(values=c(21,22)) + guides(size = FALSE, alpha = FALSE) # And now grouped ggplot(aqd_mock, aes(x=Well, y=Fe)) + geom_boxplot(aes(fill = factor(Season), alpha = .6)) + geom_point(aes(fill=Season ,size = 5, shape = factor(Aquifer), alpha = .6), colour = Black , position = position_jitterdodge()) + scale_shape_manual(values=c(21,22)) + guides(size = FALSE, alpha = FALSE, fill=guide_legend(title= Season ), shape = guide_legend(title= Aquifer )) Question What is the parameter \"position\" good for? In the beginning we talked about pretty figures, I do not know about you, but I strongly dislike the default ggplot2 theme (you should as well). How can we get rid of this grey background and white gridlines? last_plot() + theme_bw() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) That is better, at least a bit. Now we want to add/modify in additon the title and the axis labels of the plot. last_plot() + labs(title= AquaDiva mock data , subtitle= Iron content , y= Fe [mg/L] , x= Well , caption= (c) CEW ) Not bad, really not bad. The last_plot() function is incredibly useful when you are continuously working on a figure. BUT, what do we see our seasons are not properly ordered. Even in a world of climate change, autumn is not before spring and summer. We covered that yesterday. Exercise Fix the order of the seasons. Here we go. seasons - ( Spring , Summer , Autumn , Winter ) aqd_mock$Season = factor(aqd_mock$Season, levels = seasons) Given that we have the package RColorBrewer loaded, we can also fairly easy manipulate the colors of our boxplot. # Playing around with colors thanks to RColorBrewer last_plot() + scale_fill_brewer(palette= Set1 ) last_plot() + scale_fill_brewer(palette= Set2 ) last_plot() + scale_fill_brewer(palette= Blues ) Facetting and wrapping You have probably noticed that we are obviously dealing with a dataset that comprises a bunch of different variables. Lets shortly talk about types of variables, what types come to your mind? Some examples: Type Description categorical variables that can be put in categories, e.g. male and female discrete variables that are limited to a certain number of values, e.g. grades in school measurement variables that can be measured and given a number, e.g. ... ordinal categorical variables that can be ordered, e.g. low, medium, high diversity ranked ordinal variables where every point can be ordered, e.g. OTU ranks Imagine the following scenario, we now know that there are differences across season for the Fe-content in the different wells. Now we want to do the same plot for all our measurement variables. How do we do that?! What we have to do is known as facetting. For that we can make use of two different ggplot2 functions, facet_wrap() and facet_grid(), with the differencing being the number of facetting dimensions. A quick example for facet_wrap(): # We load the ggplot2 dataset mtcars data(mtcars) # And do some wrapping ggplot(data = mtcars, mapping = aes(x = hp, y = mpg)) + geom_point() + facet_wrap(~ cyl, scales = 'free_x') And for facet_grip(): ggplot(data = mtcars, mapping = aes(x = hp, y = mpg)) + geom_point() + facet_grid(am ~ cyl, scales = 'free_x') For detailed examples about facetting I recommend these links click me and me too . All nice and well, but how do we apply that to our mock data? head(aqd_mock) We have a couple of rather descriptive variables (Well, Cluster, Zone, Aquifer) and a bunch of measurement variables (e.g. PH, DO, Na, TIC, TOC, DOC). So in principle what we want to do now is transform our data in a way that allows us to facet the data based on our measurement variables. Here we go: # We use the gather function # Option (1) aqd_long - aqd_mock % % gather(Parameter, value, TEMP_W_ES:Na) head(aqd_long) # Option (2) aqd_long - gather(aqd_mock, Parameter, value, TEMP_W_ES:Na) head(aqd_long) Question Take a moment and try to figure out how gather() works. What is the role of the % % operator? head(aqd_long) Well Month Season Cluster Zone Aquifer Parameter value 1 H41 Mar Spring 2 HTL HTL TEMP_W_ES 4.4000 2 H41 Jun Summer 2 HTL HTL TEMP_W_ES 4.5000 3 H41 Aug Summer 2 HTL HTL TEMP_W_ES 4.7000 4 H41 Nov Autumn 2 HTL HTL TEMP_W_ES 4.8000 5 H41 Feb Winter 2 HTL HTL TEMP_W_ES 3.9000 6 H41 May Spring 2 HTL HTL TEMP_W_ES 4.8000 7 H41 July Summer 2 HTL HTL TEMP_W_ES 5.1000 8 H41 Oct Autumn 2 HTL HTL TEMP_W_ES 4.8000 9 H41 Jan Winter 2 HTL HTL TEMP_W_ES 4.9000 10 H41 Apr Spring 2 HTL HTL TEMP_W_ES 4.8000 Our data was partially transformed into what is commonly known as long format. We can now use the transformed dataframe for facetting as before: ggplot(aqd_long, aes(x = Well, y = value), xlab= ) + geom_point() + labs(title= Dataset parameters ) + theme(axis.text.x = element_text(angle = 25, hjust = 1)) + facet_wrap(~Parameter) That kind of worked, success with the facetting, but this plot needs some serious tweaking in terms of visual appeal. Exercise Adjust the axis ranges by free scaling Add boxplots as you did before Use the black and white theme and get rid of the grid lines etc. A potential solution could look as follows: ggplot(aqd_long, aes(x=Well, y=value)) + geom_boxplot(aes(fill = Season)) + geom_point(aes(fill = Season, alpha = 0.6, shape = factor(Aquifer)), colour = Black , position = position_jitterdodge()) + labs(title= Dataset parameters ) + theme(axis.text.x=element_text(angle = 25, hjust = 1)) + theme_bw() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + facet_wrap(~Parameter, scales = free ) + ylab(label = mg/L ) + guides(size = FALSE, alpha = FALSE, fill=guide_legend(title= Season ), shape = guide_legend(title= Aquifer )) + scale_shape_manual(values=c(21,22)) Although we look at quite a number of parameters the figure does not look terribly messy. Let's filter the dataset anyway a bit. Imagine we are only interested in the following parameters: Fe TEMP_W_ES Exercise Subset the data accordingly and re-do the plotting. aqd_long_subset - aqd_long[which(aqd_long$Parameter == c( Fe , TEMP_W_ES )),] Steps beyond Going beyond the boxplot - correlation plots One of the strong suits of ggplot2 is that it comprises numerous vizualization options, with dot plots and box plots being only two of them. And beyond that there are meanwhile a lot of R packages that stick to the ggplot2 syntax that expand R's plotting capabilities even more. Ok, what we will try now is to get in a very simple way an idea whether our parameters are correlated with each other. Quick reminder, part of our data is made up by descriptive variables. So first, we will extract only our measurement variables. # What are the dimensions of our dataframe dim(aqd_mock) # Quickly check again which columns are containing descriptive variables head(aqd_mock) # Subset the dataframe accordingly aqd_num - aqd_mock[7:24] Done. Now we calculate correlations between all measurement variables. # Some more necessary R packages library( ellipses ) library( RColorBrewer ) # Calculate correlations aqd_cor = cor(aqd_num) # A sneek peek at our correlations aqd_cor # We want to colorize our planned correlation plot, so lets create a palette my_colors - brewer.pal(5, Spectral ) my_colors=colorRampPalette(my_colors)(100) # Plot the plot ;-) ord - order(aqd_cor[1, ]) aqd_ord = aqd_cor[ord, ord] plotcorr(aqd_ord , col=my_colors[data_ord*50+50] , mar=c(1,1,1,1) ) Oh hallo, that's pretty, what does it mean? Let's break up these lines. # Calculate correlations aqd_cor = cor(aqd_num) Question What does cor() do? That was an easy one. What about: # We want to colorize our planned correlation plot, so lets create a palette my_colors - brewer.pal(5, Spectral ) my_colors=colorRampPalette(my_colors)(100) Question What is a palette? What do brewer.pal() and colorRampPalette() do? # Plot the plot ;-) ord - order(aqd_cor[1, ]) aqd_ord = aqd_cor[ord, ord] plotcorr(aqd_ord , col=my_colors[aqd_ord*50+50], type = lower , diag = FALSE, numbers = TRUE , mar=c(1,1,1,1)) Exercise Take a moment and try to figure out what the figure shows you Going beyond the boxplot while going back Last but not least we want to take a look at interactive plots. Plots do not have to static, interactive plots allow us to dive into data into a much more engaging way. Good, what we now try is to turn or facetted box plot into an interactive version. Luckily, this is extremly easy. p - ggplot(aqd_long, aes(x=Well, y=value)) + geom_boxplot() + geom_point(aes(fill = Season, alpha = 0.6, shape = factor(Aquifer)), colour = Black , position = position_jitterdodge()) + labs(title= Dataset parameters ) + theme(axis.text.x=element_text(angle = 25, hjust = 1)) + theme_bw() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + facet_wrap(~Parameter, scales = free ) + ylab(label = mg/L ) + guides(size = FALSE, alpha = FALSE, fill=guide_legend(title= Season ), shape = guide_legend(title= Aquifer )) + scale_shape_manual(values=c(21,22)) p - ggplotly(p) p You are now able to explore your data interactively in the viewer window of RStudio. For more ideas about interactive ggplot2 plots check out this link . Exercise Create interactive boxplots: For a single variable boxplot (e.g. Fe) And a subset Written by Carl-Eric Wegner Oct 2018","title":"Data visualization and exploration"},{"location":"part3/#data-visualization-and-exploration","text":"","title":"Data visualization and exploration"},{"location":"part3/#synopsis","text":"In this part of the workshop you will familiarize yourself with: R's basic plotting capabilities ggplot2 , its syntax and fundamentals as well as some advanced data visualization If you have questions ASK, feel free to drop me an e-mail also after the workshop.","title":"Synopsis"},{"location":"part3/#setting-the-stage","text":"Plotting is a very personal thing (btw color schemes as well), ask three different people and you will get a variety of feedback with respect to plotting and data visualization in general. A lot of people get away with Excel , SigmaPlot , GraphPad , Matplotlib (if you are a Pythonista, which makes you a good person by default) or something else. R as its parent S is a programming language primarily dedicated to data manipulation (in a good sense) and statistical analysis. Due to its modular architecture and the availabilitry of sophisticated libraries for data processing and plotting it is an excellent choice for any kind of data visualization. When I got in touch with R the first time, I was googling options how to do a complex multi-panel figure. The original query was \"plot facets vector graphics\". If you do the exact same query now, you will end up with ggplot2 being the second hit. Yep, this is how I \"met\" R and years later I would still lie if I would say that I'm any sort of R expert. Most of me using R is still centered around the following dogma: Define the problem Look up necessary R resources Apply available examples to own data Be happy after A LOT of try and error. Ok, first things first, fire up RStudio and check whether you have the following packages installed and load them. Throughout the course you can either execute commands from within a script (mark the respective lines of code and hit CTRL+ENTER) or directly in the console. library( ggplot2 ) library( dplyer ) library( tidyr ) library( ellipse ) library( RColorBrewer ) Ideally you should see no error message popping up, if you see any then you did not prepare properly for the workshop - shame on you, or rather me, because I did not tell you in time.","title":"Setting the stage"},{"location":"part3/#basic-plotting","text":"Enough blabla, this session is about data visualization, where are the plots? Here they come. Create a new R script, type the following two lines and execute them: dotchart(rnorm(250), col = blue , main = Quick ugly example ) hist(rnorm(250), col = blue , main = Quick ugly example II ) Alternatively type them one after another in the console. Keep an eye on the plot window, what did just happen? Question Try to figure out what the individual functions and parameters do. First take home message of the day - the R help is your friend (a good one). The general syntax for calling R's help is: ?functionXYZ() Let's start exploring R's basic plotting capabilities in a bit more detail, this is a bit of a recap of what you did yesterday. # Let's define two arbitrary vectors bacteria - c(10, 30, 60, 5, 90) archaea - c(25, 27, 22, 37, 10) # Plot them both plot(bacteria, type= o , col = orange ) lines(archaea, type= o , pch=22, lty=2, col = blue ) axis(1, at=1:5, lab=(c( March , April , May , June , July ))) Let's say what we just plotted are relative abundances for archaea and bacteria across different months. But the output looks like garbage, apparently we had at first default x-axis labels, which were overwritten. The result is this \"beauty\" of a plot. How to fix that, take a look at the following code: plot(bacteria, type= o , col = orange , axes=FALSE, ann = FALSE) lines(archaea, type= o , pch=22, lty=2, col = blue ) axis(1, at=1:5, lab=(c( March , April , May , June , July ))) Does that make it any better? What is now missing? Exercise Call the help for axis(), box(), title(), and legend. Add a y-axis, a box around the plot, titles for the plot as well as the two axes, and a legend. One possible solution: plot(bacteria, type= o , col = orange , axes=FALSE, xlab = Month , ylab = Rel. abundance [%] , main = Bac and Arc ) lines(archaea, type= o , pch=22, lty=3, col = blue ) axis(1, at=1:5, lab=(c( March , April , May , June , July ))) box() axis(2, las = 2) legend(1, max(bacteria), c( Bacteria , Archaea ), cex=0.8, col=c( orange , blue ), pch=c(21,22), lty=c(1,3)) Alright, so far we played with a dataset that we quickly created, as you already learned before you can easily import datasets like this from any delimited file. Imagine a file like this (e.g. table.tsv): bacteria archaea 10 25 30 27 60 22 5 37 90 10 Two columns, tab-delimited, and the columns have names (\"bacteria2, \"archaea\"). We could read this file as outlined below. # Read the table, pay attention to the header and sep arguments rel_prok - read.table( table.tsv , header=T, sep= \\t ) # Instead we merge our two vectors because you did a lot of importing yesterday... rel_prok - data.frame(bacteria, archaea) colnames(rel_prok) - c(bacteria, archaea) # We define colors to be used with our data series, because why not plot_colors - c( blue , orange ) # We initiate a PNG devide to save the output png(filename= output.png , height=250, width=300, bg= white ) # AND NOW?! # Adapt your code # and end it with dev.off() # to turn off the PNG device Exercise Plot the data as before, and save the output as a .png. Do you have to adjust the dimensions? What is the dev.off() function doing?","title":"Basic plotting"},{"location":"part3/#base-r-data-visualization-options","text":"So far we basically only did line charts, base R provides us however with a whole range of different visualization options. Let's take one of our vectors and see how we can create bar charts and how we can visualize both data series by dot charts. Bar charts: bacteria - c(10, 30, 60, 5, 90) # A simple bar plot barplot(bacteria, main= Bacteria relative abundance , xlab= Month , ylab= Rel. abundance [%] , names.arg=c( March , April , May , June , July )) # Simple, and slightly pimped, pattern fill barplot(bacteria, main= Bacteria relative abundance , xlab= Month , ylab= Rel. abundance [%] , names.arg=c( March , April , May , June , July ), border= gray , density=c(10,20,30,40,50)) # Add a box around the plot because we like boxes box() # This time with colors barplot(bacteria, main= Bacteria relative abundance , xlab= Month , ylab= Rel. abundance [%] , names.arg=c( March , April , May , June , July ), col=rainbow(5)) And a dot chart: # Plot the dotchart dotchart(t(rel_prok), color=c( blue , red ), main= Dotchart Bacteria and Archaea ) We finish this first session with a little exercise. Exercise Use the simple dataset to plot a grouped bar chart incl. a legend and a dot chart with months as row names. Export both as .png files. NOT using RStudio's export function. For more examples of using the plotting capabilities of base R have a look for instance here . Solutions: # Grouped bar chart barplot(as.matrix(rel_prok), main= Bac vs Arc , ylab= Rel. abundance , beside=TRUE, col=rainbow(5)) box() # Place the legend at the top-left corner with no frame # using rainbow colors legend( topleft , c( March , April , May , June , July ), cex=1, bty= n , fill=rainbow(5)) # Dot chart with months as labels row.names(rel_prok) - c( March , April , May , June , July ) rel_prok dotchart(t(rel_prok), color=c( blue , red ), main= Dotchart Bacteria and Archaea , cex = 1)","title":"Base R data visualization options"},{"location":"part3/#basic-usage-of-ggplot2","text":"","title":"Basic usage of ggplot2"},{"location":"part3/#ggplot2","text":"Base R's plotting capabilities are not bad, but the bottom line is, even with a lot of tweaking the resulting plots are visually rather less appealing. When you are dealing with your data, you want to present it in the best possible/convincing way. Often when I read papers and I see mediocre figures I think one thing, namely: \"RESPECT YOUR DATA!\". Investing time in in proper plotting/visualization/beautifying is usually more than worth it as it pays off in multiple regards. Luckily, there are a multitude of R packages that provide us with almost unlimited options of data visualization. The most common one is ggplot2 , which was created and is maintained by Hadley Wickham . Hadley is incredibly active in the R scene and maintains a lot of popular R packages/tools including: dplyr tidyr stringr ggplot2 to name a few. He is also one of the main persons behind RStudio .","title":"ggplot2"},{"location":"part3/#ggplot2-versus-base-r","text":"Plotting in base R can be mostly done using data stored in vectors. In comparison, ggplot2 relies on dataframes. Let's have a look at one of these as a quick reminder: data(mtcars) head(mtcars) mpg cyl disp hp drat wt qsec vs am gear carb Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Dataframes are nothing else but lists of vectors of equal lengths. It we take a qick look at our workshop mock data, are we dealing with a dataframe as well? aqd_mock - read.table( simulated_dataset.txt , header=T, sep= \\t , row.names = SampleID ) head(aqd_mock) Well Month Season Cluster Zone Aquifer TEMP_W_ES EC EC25 PH DO NH4 PO4 DOC TOC TIC NO3 SO4 Cl Ca Sample1 H41 Mar Spring 2 HTL HTL 4.4 455 113.7800 7.41 6.41 0.0 0.4 1.3 1.3 36.81 24.34 21.86 5.6 34.7 Sample2 H41 Jun Summer 2 HTL HTL 4.5 465 113.2046 8.21 5.50 0.1 0.1 1.1 1.2 37.45 19.29 15.33 5.8 33.7 Sample3 H41 Aug Summer 2 HTL HTL 4.7 444 108.1701 7.31 3.71 0.2 0.9 1.7 1.8 32.23 12.98 19.24 5.6 30.9 Sample4 H41 Nov Autumn 2 HTL HTL 4.8 449 110.1839 7.31 3.20 0.2 0.1 1.5 1.7 35.84 12.04 16.49 5.7 30.3 Sample5 H41 Feb Winter 2 HTL HTL 3.9 457 115.9376 7.21 6.99 0.0 1.1 0.7 0.8 34.27 23.79 26.22 5.9 35.4 Sample6 H41 May Spring 2 HTL HTL 4.8 477 116.3691 7.31 7.83 0.0 2.4 0.7 0.7 35.37 26.71 23.91 5.6 36.0 Fe Mg Mn Na Sample1 0.0 23.17 0.00 3.0 Sample2 0.0 24.51 0.00 2.0 Sample3 5.2 29.54 0.35 3.5 Sample4 1.4 29.00 0.34 3.3 Sample5 6.0 23.90 0.09 3.5 Sample6 2.7 21.95 0.09 1.6 Apparently we do. BTW: Question Why do we add the \"header\" and \"row.names\" parameters? Besides from the usage of dataframes, the second key characteristics of ggplot2 is that you work with layers. Basically, every ggplot2 object is like a canvas and we keep painting on it by adding layers, aka geoms. library( ggplot2 ) # Initialize a ggplot object ggplot(aqd_mock, aes(x=Zone, y=Fe)) Thats our canvas, and yes so far it is fairly empty. We have to fill it by adding the aforementioned geom objects, before we take a look at the overall ggplot2 syntax. ggplot( data = some_data_frame , mapping = aes( x = some_column , y = some_other_column , random aesthetics = based_on_a_random_parameter ) ) + geom_ some_plot_type () Now we add some content. ggplot(aqd_mock, aes(x=Zone, y=Fe)) + geom_point() We just created our first ggplot2 plot. Whoa.","title":"ggplot2 versus base R"},{"location":"part3/#basic-customizations-a-bit-about-aesthetics","text":"Lets be more serios, our dataset spans data from different seasons. One obvious question is whether we can identify differences over the year? Let's find out and color our dots according to season. # A touch of color ggplot(aqd_mock, aes(x=Well, y=Fe, col = Season)) + geom_point() We can use this simple example to learn more about how ggplot2 aesthetics work. ggplot(aqd_mock, aes(x=Zone, y=Fe)) + geom_point(aes(col = Season)) ggplot(aqd_mock, aes(x=Zone, y=Fe, col = Season)) + geom_point(colour = Black ) # Why does the following not work? ggplot(aqd_mock, aes(x=Zone, y=Fe, col = Season)) + geom_point(aes(col = Black )) # A second variable to modify aesthetics ggplot(aqd_mock, aes(x=Zone, y=Fe)) + geom_point(aes(col = Season, shape = Well)) # That does not make so much sense hm? ggplot(aqd_mock, aes(x=Zone, y=Fe)) + geom_point(aes(col = Season, shape = Aquifer)) ggplot(aqd_mock, aes(x=Zone, y=Fe)) + geom_point(aes(fill = Season, shape = factor(Aquifer), alpha = .6, size = 5), colour = Black ) + scale_shape_manual(values=c(21,22)) # Getting rid of some legends ggplot(aqd_mock, aes(x=Zone, y=Fe)) + geom_point(aes(fill = Season, shape = factor(Aquifer), alpha = .6, size = 5), colour = Black ) + scale_shape_manual(values=c(21,22)) + guides(size = FALSE, alpha = FALSE) # Success Ah well, so far nothing really obvious, however right now it is really hard to tell. Let's try to visualize this better. Exercise Check out geom_boxplot() via ?geom_boxplot() Plot box plots combined with dot plots for the Fe content at the different wells, using the season as grouping. Try out geom_violin() as well. Does that that help, do we see differences across season now? # A first boxplot ggplot(aqd_mock, aes(x=Well, y=Fe)) + geom_boxplot(alpha = .6) + geom_point(aes(fill=Season ,size = 5, shape = factor(Aquifer), alpha = .6), colour = Black , position = position_jitterdodge()) + scale_shape_manual(values=c(21,22)) + guides(size = FALSE, alpha = FALSE) # And now grouped ggplot(aqd_mock, aes(x=Well, y=Fe)) + geom_boxplot(aes(fill = factor(Season), alpha = .6)) + geom_point(aes(fill=Season ,size = 5, shape = factor(Aquifer), alpha = .6), colour = Black , position = position_jitterdodge()) + scale_shape_manual(values=c(21,22)) + guides(size = FALSE, alpha = FALSE, fill=guide_legend(title= Season ), shape = guide_legend(title= Aquifer )) Question What is the parameter \"position\" good for? In the beginning we talked about pretty figures, I do not know about you, but I strongly dislike the default ggplot2 theme (you should as well). How can we get rid of this grey background and white gridlines? last_plot() + theme_bw() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) That is better, at least a bit. Now we want to add/modify in additon the title and the axis labels of the plot. last_plot() + labs(title= AquaDiva mock data , subtitle= Iron content , y= Fe [mg/L] , x= Well , caption= (c) CEW ) Not bad, really not bad. The last_plot() function is incredibly useful when you are continuously working on a figure. BUT, what do we see our seasons are not properly ordered. Even in a world of climate change, autumn is not before spring and summer. We covered that yesterday. Exercise Fix the order of the seasons. Here we go. seasons - ( Spring , Summer , Autumn , Winter ) aqd_mock$Season = factor(aqd_mock$Season, levels = seasons) Given that we have the package RColorBrewer loaded, we can also fairly easy manipulate the colors of our boxplot. # Playing around with colors thanks to RColorBrewer last_plot() + scale_fill_brewer(palette= Set1 ) last_plot() + scale_fill_brewer(palette= Set2 ) last_plot() + scale_fill_brewer(palette= Blues )","title":"Basic customizations / a bit about aesthetics"},{"location":"part3/#facetting-and-wrapping","text":"You have probably noticed that we are obviously dealing with a dataset that comprises a bunch of different variables. Lets shortly talk about types of variables, what types come to your mind? Some examples: Type Description categorical variables that can be put in categories, e.g. male and female discrete variables that are limited to a certain number of values, e.g. grades in school measurement variables that can be measured and given a number, e.g. ... ordinal categorical variables that can be ordered, e.g. low, medium, high diversity ranked ordinal variables where every point can be ordered, e.g. OTU ranks Imagine the following scenario, we now know that there are differences across season for the Fe-content in the different wells. Now we want to do the same plot for all our measurement variables. How do we do that?! What we have to do is known as facetting. For that we can make use of two different ggplot2 functions, facet_wrap() and facet_grid(), with the differencing being the number of facetting dimensions. A quick example for facet_wrap(): # We load the ggplot2 dataset mtcars data(mtcars) # And do some wrapping ggplot(data = mtcars, mapping = aes(x = hp, y = mpg)) + geom_point() + facet_wrap(~ cyl, scales = 'free_x') And for facet_grip(): ggplot(data = mtcars, mapping = aes(x = hp, y = mpg)) + geom_point() + facet_grid(am ~ cyl, scales = 'free_x') For detailed examples about facetting I recommend these links click me and me too . All nice and well, but how do we apply that to our mock data? head(aqd_mock) We have a couple of rather descriptive variables (Well, Cluster, Zone, Aquifer) and a bunch of measurement variables (e.g. PH, DO, Na, TIC, TOC, DOC). So in principle what we want to do now is transform our data in a way that allows us to facet the data based on our measurement variables. Here we go: # We use the gather function # Option (1) aqd_long - aqd_mock % % gather(Parameter, value, TEMP_W_ES:Na) head(aqd_long) # Option (2) aqd_long - gather(aqd_mock, Parameter, value, TEMP_W_ES:Na) head(aqd_long) Question Take a moment and try to figure out how gather() works. What is the role of the % % operator? head(aqd_long) Well Month Season Cluster Zone Aquifer Parameter value 1 H41 Mar Spring 2 HTL HTL TEMP_W_ES 4.4000 2 H41 Jun Summer 2 HTL HTL TEMP_W_ES 4.5000 3 H41 Aug Summer 2 HTL HTL TEMP_W_ES 4.7000 4 H41 Nov Autumn 2 HTL HTL TEMP_W_ES 4.8000 5 H41 Feb Winter 2 HTL HTL TEMP_W_ES 3.9000 6 H41 May Spring 2 HTL HTL TEMP_W_ES 4.8000 7 H41 July Summer 2 HTL HTL TEMP_W_ES 5.1000 8 H41 Oct Autumn 2 HTL HTL TEMP_W_ES 4.8000 9 H41 Jan Winter 2 HTL HTL TEMP_W_ES 4.9000 10 H41 Apr Spring 2 HTL HTL TEMP_W_ES 4.8000 Our data was partially transformed into what is commonly known as long format. We can now use the transformed dataframe for facetting as before: ggplot(aqd_long, aes(x = Well, y = value), xlab= ) + geom_point() + labs(title= Dataset parameters ) + theme(axis.text.x = element_text(angle = 25, hjust = 1)) + facet_wrap(~Parameter) That kind of worked, success with the facetting, but this plot needs some serious tweaking in terms of visual appeal. Exercise Adjust the axis ranges by free scaling Add boxplots as you did before Use the black and white theme and get rid of the grid lines etc. A potential solution could look as follows: ggplot(aqd_long, aes(x=Well, y=value)) + geom_boxplot(aes(fill = Season)) + geom_point(aes(fill = Season, alpha = 0.6, shape = factor(Aquifer)), colour = Black , position = position_jitterdodge()) + labs(title= Dataset parameters ) + theme(axis.text.x=element_text(angle = 25, hjust = 1)) + theme_bw() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + facet_wrap(~Parameter, scales = free ) + ylab(label = mg/L ) + guides(size = FALSE, alpha = FALSE, fill=guide_legend(title= Season ), shape = guide_legend(title= Aquifer )) + scale_shape_manual(values=c(21,22)) Although we look at quite a number of parameters the figure does not look terribly messy. Let's filter the dataset anyway a bit. Imagine we are only interested in the following parameters: Fe TEMP_W_ES Exercise Subset the data accordingly and re-do the plotting. aqd_long_subset - aqd_long[which(aqd_long$Parameter == c( Fe , TEMP_W_ES )),]","title":"Facetting and wrapping"},{"location":"part3/#steps-beyond","text":"","title":"Steps beyond"},{"location":"part3/#going-beyond-the-boxplot-correlation-plots","text":"One of the strong suits of ggplot2 is that it comprises numerous vizualization options, with dot plots and box plots being only two of them. And beyond that there are meanwhile a lot of R packages that stick to the ggplot2 syntax that expand R's plotting capabilities even more. Ok, what we will try now is to get in a very simple way an idea whether our parameters are correlated with each other. Quick reminder, part of our data is made up by descriptive variables. So first, we will extract only our measurement variables. # What are the dimensions of our dataframe dim(aqd_mock) # Quickly check again which columns are containing descriptive variables head(aqd_mock) # Subset the dataframe accordingly aqd_num - aqd_mock[7:24] Done. Now we calculate correlations between all measurement variables. # Some more necessary R packages library( ellipses ) library( RColorBrewer ) # Calculate correlations aqd_cor = cor(aqd_num) # A sneek peek at our correlations aqd_cor # We want to colorize our planned correlation plot, so lets create a palette my_colors - brewer.pal(5, Spectral ) my_colors=colorRampPalette(my_colors)(100) # Plot the plot ;-) ord - order(aqd_cor[1, ]) aqd_ord = aqd_cor[ord, ord] plotcorr(aqd_ord , col=my_colors[data_ord*50+50] , mar=c(1,1,1,1) ) Oh hallo, that's pretty, what does it mean? Let's break up these lines. # Calculate correlations aqd_cor = cor(aqd_num) Question What does cor() do? That was an easy one. What about: # We want to colorize our planned correlation plot, so lets create a palette my_colors - brewer.pal(5, Spectral ) my_colors=colorRampPalette(my_colors)(100) Question What is a palette? What do brewer.pal() and colorRampPalette() do? # Plot the plot ;-) ord - order(aqd_cor[1, ]) aqd_ord = aqd_cor[ord, ord] plotcorr(aqd_ord , col=my_colors[aqd_ord*50+50], type = lower , diag = FALSE, numbers = TRUE , mar=c(1,1,1,1)) Exercise Take a moment and try to figure out what the figure shows you","title":"Going beyond the boxplot - correlation plots"},{"location":"part3/#going-beyond-the-boxplot-while-going-back","text":"Last but not least we want to take a look at interactive plots. Plots do not have to static, interactive plots allow us to dive into data into a much more engaging way. Good, what we now try is to turn or facetted box plot into an interactive version. Luckily, this is extremly easy. p - ggplot(aqd_long, aes(x=Well, y=value)) + geom_boxplot() + geom_point(aes(fill = Season, alpha = 0.6, shape = factor(Aquifer)), colour = Black , position = position_jitterdodge()) + labs(title= Dataset parameters ) + theme(axis.text.x=element_text(angle = 25, hjust = 1)) + theme_bw() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + facet_wrap(~Parameter, scales = free ) + ylab(label = mg/L ) + guides(size = FALSE, alpha = FALSE, fill=guide_legend(title= Season ), shape = guide_legend(title= Aquifer )) + scale_shape_manual(values=c(21,22)) p - ggplotly(p) p You are now able to explore your data interactively in the viewer window of RStudio. For more ideas about interactive ggplot2 plots check out this link . Exercise Create interactive boxplots: For a single variable boxplot (e.g. Fe) And a subset Written by Carl-Eric Wegner Oct 2018","title":"Going beyond the boxplot while going back"},{"location":"part4/","text":"Basic Statistical Tests with R This is going to be fairly fast paced and brief discussion of many commonly used statistical tests and how to run them in R. I barely scratch the surface in terms of the kinds of tests available and which should be used. I HIGHLY HIGHLY recommend you check out the Handbook of Biological Statistics and the R companion book (both free online) when you are analyzing your own data!! Some extremely excellent statistic references Handbook of Biological Statistics http://www.biostathandbook.com/ R Companion for Biological Statistics https://rcompanion.org/rcompanion/a_02.html Intro R Statistics https://www.bioinformatics.babraham.ac.uk/training/R_Statistics/Introduction%20to%20Statistics%20with%20R%20manual.pdf Probability Statistics https://cran.r-project.org/web/packages/IPSUR/vignettes/IPSUR.pdf Handbook of Statistical Analyses https://cran.r-project.org/web/packages/HSAUR/vignettes/Ch_introduction_to_R.pdf Our plan today Today we will discuss and then explore the following ideas: - Common assumptions what these look like. - Students T tests and derivations - ANOVA and derivations - Linear Regression / Correlation Assumptions of common parametric tests: Independence The measurements collected are independent from each other. Common voiolations include: timeseries data repeated measurements from the same sample measurements from small spatial region Other? Normality Do the measurements come from a normal or gaussian distribution? Surprisingly, this is actually not that important since most of the common parametric tests are fairly robust to deviations from normality You can try to transform data to meet assumptions of normality A great idea is to plot the data and see what you are getting yourself into Homoscedasticity Or the assumption that different groups have the same standard deviation more of a problem if you have an unbalanced dataset again best to plot you data and get a feel for what it looks likes An example: #Loading the libraries we will be using today library(ggplot2) library(dplyr) #An example dataset ex_norm - data.frame(sampleid = seq(1,100), y = rnorm(n=100, mean=10, sd=2), treatment=rep(c( A , B ), each=50)) #Our class dataset from yesterday df - df - read.delim( simulated_dataset.txt , header=T) Lets start exploring these data! #Plotting a histogram to see how your univariate data are arranged ggplot(ex_norm, aes(y))+ geom_histogram(binwidth = 0.5) #Using a scatter plot to look at your data ggplot(ex_norm, aes(y))+ geom_point(aes(x = treatment, y = y)) #Using a box plot ggplot(ex_norm, aes(x=treatment, y=y))+ geom_boxplot(aes(y=y)) #Fancier box plot (violin plot) ggplot(ex_norm, aes(x=treatment, y=y))+ geom_violin(aes(y=y)) Working with some of our data: The joys of real life ggplot(df, aes(NO3))+ geom_histogram() ggplot(df, aes(NO3))+ geom_histogram(binwidth = 5) ggplot(df, aes(NO3))+ geom_histogram(aes(fill = Well)) ggplot(df, aes(x=Well, y = NO3))+ geom_point() ggplot(df, aes(x=Well, y = NO3))+ geom_boxplot() So what do you think? Are these groups normally distributed? We can use the Shapiro Wilk normality test to see. Just FYI, this test is quite sensitive to non-normal data. If it fails, you don't have to immediately panic since the following tests we'll talk about are much more robust to non-normal data. df % % select(Well,NO3) % % group_by(Well) % % summarise(statistic = shapiro.test(NO3)$statistic, p.value = shapiro.test(NO3)$p.value) df % % select(Well,NO3) % % group_by(Well) % % summarise(statistic = shapiro.test(sqrt(NO3))$statistic, p.value = shapiro.test(sqrt(NO3))$p.value) #This test is running the following command on each of the Wells shapiro.res - shapiro.test(df[df$Well == H43 ,]$NO3) shapiro.res$statistic shapiro.res$p.value Is the sampling scheme balanced? df % % select(Well,NO3) % %group_by(Well) % % summarise(n=n()) What can we do? #squareroot transform ggplot(df, aes(x = Well, y = sqrt(NO3)))+ geom_point()+ stat_summary(fun.y = mean , geom = point , color= blue ) #log10 transform ggplot(df, aes(x = Well, y = log10(NO3+0.5)))+ geom_point()+ stat_summary(fun.y = mean , geom = point , color= blue ) Which looks better? #Note, this test is sensitive to non-normal data as well as heteroscedastic data. #It is nice for picking the best transform though. #raw data bartlett.test(data=df, NO3 ~ Well) #squareroot transform bartlett.test(data=df, sqrt(NO3) ~ Well) #log10 transform bartlett.test(data=df, log10(NO3+1) ~ Well) OK, it is your turn. Individually or in small groups, check other variables in the data frame Ask yourself if: - normal - homoscedastic - independent - transform Make sure you look at some of the variables associated with the aquifer. Student's t tests One-sample t tests These are used to compare the mean of your population to a theoretical mean #Lets see if the Nitrate from well H43 has a mean of 0. t.test(df[df$Well == H43 ,]$NO3, mu = 0, conf.level = 0.95) #We know that nitrate violates some assumptions of normality. #How does the more normal square root transformed data appear? t.test(sqrt(df[df$Well == H43 ,]$NO3), mu = 0, conf.level = 0.95) #Always a good idea to see if you stats agree with your intuition of the data ggplot(df[df$Well == H43 ,], aes(x=Well, y=NO3))+ geom_point(position = position_jitter()) #What happens with our example dataset? t.test(ex_norm$y, mu = 10, conf.level = 0.95) OK, now it is your turn, test to see if the lower aquifer (HTL) is anaerobic. Two-sample t tests These are much more common (in my opinion). Here we are comparing the means between 2 groups. Lets see if the 1st half of our simulated normal data is significantly different from the last half. t.test(ex_norm[seq(1,50),]$y, ex_norm[seq(51,100),]$y) #What is this actually doing? #Lets plot our data again ggplot(df, aes(x = Aquifer, y = NO3))+ geom_boxplot() #Check for equal variances bartlett.test(data=df, NO3 ~ Aquifer) #Run the two-sample t.test based on the aquifer t.test(df[df$Aquifer == HTL ,]$NO3, df[df$Aquifer == HTU ,]$NO3) In general though, if you have a large enough sample size (approximately 5) you want to use a Welchs t test (default in R and what we've been using). This variation does not assume equal variance. In fact, many places recommend to always use this test over a students t test. If the samples have the same variance you can turn that on in this way t.test(df[df$Aquifer == HTL ,]$NO3, df[df$Aquifer == HTU ,]$NO3, var.equal = TRUE) If you do not think you samples are normally distributed you can use a Mann Whitney U test. wilcox.test(df[df$Aquifer == HTL ,]$NO3, df[df$Aquifer == HTU ,]$NO3) #Note: We get the cannot compute exact p-values since we have multiple measurements #of the same value (0) in the HTU dataset. In practice, I almost always use the default Welch's t.test for un-equal variance. ANOVA One-way ANOVA We can use an analysis of variance (ANOVA) test to compare the means of 2 or more groups. If it is only 2 groups, then a one-way anova and a two-sample t test are identical. There are A LOT of different ways to do ANOVA tests and we will quickly cover a few. As with everything in this section, I highly recommend you double check references before you get started. no3_aov - aov(data=df, sqrt(NO3) ~ Well) summary(no3_aov) Understanding the output: ANOVA essentially tests if the variance between your group means is different from the variance within the groups. Under the null hypothesis, all the groups have a same mean, so the variance between those group means will be the same as the average within group variation. There are 100s of books written on ANOVA and I recommend checking some of them out. Might also be good to manually calculate the values yourself once if you are interested. The ratio of these variances under the null fits an F-distribution (hence the F value). The p.value can then be determined from how different our tests F value is from the expected F value under the null hypothesis. The residuals in an R anova table refer to the within group statistics. The degrees of freedom are \"number observations\" - \"number of groups\". The group variable is specified (between-group means) and the degrees of freedom here are calculated as the \"number of groups\" - 1. Ideally you report the results as Nitrate concentrations in the wells were significantly different (one-way ANOVA, F 2,27 = 29, p 1e-7). Note, ANOVA assumes your group data are normally distributed, with equal standard deviations in the groups, and are the measurements are independent. However ANOVA is fairly robust to non-normal data and if your sampling scheme is balanced it is robust to differences in standard deviations between the groups as well. If your experimental plan is not balanced, you need to be more careful as you can get a lot more false positives (p values 0.05, but no actual differences between the means). That said, if you have a very significant p value (ours is 1.71e-7) your means are probably different. A non-parametric version of ANOVA can also be used if your data are not normal. This would be the Kruskal Wallis test. It will have a lot less power and should be picked if your data are not normal AND you dont expect them to be normal. Interestingly, some people (John McDonald of the Handbook for Biological Statistics) recommend never using this test. kruskal.test(data=df, sqrt(NO3) ~ Well) There is also a version of ANOVA if you think your data are normal, but you do not expect equal variances. oneway.test(data=df, sqrt(NO3) ~ Well) There are a couple of built in ways to check your assumptions after running the ANOVA test. #Check afterward your ANOVA for whether the groups have similar variances #Here we are plotting the residuals vs the group means. #Remember that the residuals are the data values - group mean . #There should be no relationship between the residuals and the means (flat line). #The spread should be approximately equal as well. plot(no3_aov,1) #We can also check to see if the group data are normal. plot(no3_aov, 2) #Here we are plotting quantiles of the residuals against the quantiles of a normal distribution. #Normal data will match the theoretical normal distribution giving a straight 1:1 line. OK great, now we are pretty sure that NO3 values are different between our wells. However, we want to know WHICH wells are different from each other. Remember, ANOVA only tells you if there is a diffence in your groups means and NOT which groups are different. We can do post-hoc tests to see which groups are significantly different. The most common option that I've seen is to use Tukeys HSD (honest significant differences) test. This assumes equal variances between the groups, and corrects for multiple tests. TukeyHSD(no3_aov) There are no base R functions for unbalanced / unequal variances for post-hoc testing. There are other packages you can try though (multcomp, lsmeans, DescTools, agricolae). #You can also do two-way t tests and correct the p.values for multiple tests. t.test(sqrt(df[df$Well == H52 ,]$NO3), sqrt(df[df$Well == H41 ,]$NO3))$p.value t.test(sqrt(df[df$Well == H52 ,]$NO3), sqrt(df[df$Well == H43 ,]$NO3))$p.value t.test(sqrt(df[df$Well == H43 ,]$NO3), sqrt(df[df$Well == H41 ,]$NO3))$p.value p.adjust(c(0.008654862, 0.00662454, 1.876758e-10), method= bonferroni ) Pick a different variable, and test the means. Make sure to check how badly it violates the assumptions. Two-way ANOVA You can use a two-way ANOVA when there are two parameters (factors) associated with your data. In our case, an example would be \"Well\" and \"Season\". We have sampled our 3 wells multiple times throughout the year and over multiple years. We can use a two-way ANOVA to test whether our measured data is significantly different between well, between season, and if there is a well specific seasonal effect (intereaction between our factors). Two-way ANOVA is often used for repeated measurements from the same individuals and you want to control for differences between those individuals. Two-ANOVA with replication tests 3 NULL hypotheses. 1. The means within one of our factors are the same, 2. The means within the other factor are the same. 3. There is no interaction between the factors. You can also do a two-way ANOVA test without replication (one data point per group), but you need to assume that there is no interaction term and the test is much weaker. Generally, if the interaction between your variables is significant then you need to change your analysis plan since it is not fair to test for significance in only one of the variables. Instead, you can split your data up and do 2 one-way ANOVA tests. Lets take a look for what we are testing: ggplot(df)+ geom_boxplot(aes(x = Well, y = sqrt(NO3), color = Season)) ggplot(df)+ geom_boxplot(aes(x = Season, y = sqrt(NO3), color = Well)) Thoughts? #Run the anova in R and save the result no3_2fac_aov - aov(data=df, sqrt(NO3) ~ Season + Well + Season:Well) #view information about the ANOVA summary(no3_2fac_aov) #Example of how R specifies ANOVA formulas summary(aov(data=df, sqrt(NO3) ~ Season + Well)) summary(aov(data=df, sqrt(NO3) ~ Season*Well)) Lets look at the assumptions: plot(no3_2fac_aov, 1) plot(no3_2fac_aov, 2) #Actually looks quite a bit better than I expected... #Might be a good idea to remove the outliers and re-test though. OK, so this is not really fair. But as an example (assuming no interaction was found) we can still do follow up testing between groups. TukeyHSD(no3_2fac_aov, which = Well ) Take some time and run a two-way ANOVA test with a different dependent variable. Linear Regression and Correlation Linear regression and correlation are used to investigate the relationship between two continuous variables. In general you want to know if two variables are related, how closely they are related, and mathematically describe this relationship. I am not going into details into the differences between linear regression and correlation. Check out this page for more info http://www.biostathandbook.com/linearregression.html Also, R is perfectly happy to let you run linear regression with 1 measurement variable and factors (discrete / categorical variables). In this case, it actually runs ANOVA/MANOVA in the background. I find this both convenient (you can use the same syntax and still be correct) and confusing (you may be running a different test than you expect). This is also why you see a lot of examples where people set up their ANOVA test with a linear regression model. For example: linear_model - lm(data=df, sqrt(NO3) ~ Well) summary(aov(linear_model)) Anways, lets see if Calcium and Sodium concentrations are related. ggplot(df, aes(x = Ca, y = Na))+ geom_point() This looks pretty clear, but lets test it. By default, R uses a pearson correlation test which assumes that the data are linearly related and that the residuals are normally distributed. The null hypothesis is that the variables are not related (slope = 0). cor.test(~ Ca + Na, data=df) A nonparametric correlation test commonly used is the Spearman rank correlation test. It does not assume a distribution for the variables of if they are related. cor.test(~ Ca + Na, data=df, method= spearman ) The correlation coefficient and the p value tend to be reported. The correlation coefficient (rho) goes from -1 to 1, where -1 indicates the variables are perfectly negatively correlated, and 1 indicates they are perfectly positively correlated. The p value is calculated using a t distribution with n-2 degrees of freedom. If we want to generate a linear regression model for these measurements, we can use the built in function lm(). lm(Ca ~ Na, data = df) The formula format R uses is Y ~ X1 The \"Y\" variable is traditionally assigned as the dependent variable while the \"X\" variable(s) are the independent ones. Our example is not great since we are looking at two dependent variables that are probably not independent. In R the lm() function fits the ordinary-least-squares regression line, or the line that minimizes the distance between your observations and the line itself. This works best for analysing two continuous variables with both independent and dependent variables. It is not valid for 2 dependent variables (assuming there is no cause and effect relationship). There are other techniques in this instance, however, they are not valid if you want to predict unmeasured values. Anyways, I strongly recommend you read the literature or use methods typically employed in your field before running these analyses on your own data. But for us, lets assume everything is fine and explore our linear regression results. linear_model1 - lm(Ca ~ Na, data = df) summary(linear_model1) #Note that the pvalue the t.value for the slope are the same as we got from the Pearson correlation analysis. cor.test(~ Ca + Na, data=df) The r^2 value (coefficient of determination) is a measure of how well your regression line fits the data, or how much of the variance of your Y variable (dependent) is explained by the X variable. Values close to 1 indicate your observed Y values fall on your regression line while values close to 0 indicate there is no relationship between your variables. Like with ANOVA, we can explore data characteristics from the linear regression results. #Check for homoscedascity bias (line should be flat around 0) plot(linear_model1,1) #Check for normality plot(linear_model1,2) OK, lets try another example. We know that oxygen concentrations are physically dependent on temperature. Lets see if there is a relationship between these values. Which is the dependent and which is the independent variable? ggplot(df, aes(x = TEMP_W_ES, y=DO))+ geom_point() cor.test(~ TEMP_W_ES + DO, data=df) summary(lm(DO ~ TEMP_W_ES, data=df)) But, wait. Lets think about this? Does a value of 0 depend on temperature in this case? ggplot(df, aes(x = TEMP_W_ES, y=DO))+ geom_point(aes(color=Well)) #Lets look at only wells with oxygen. cor.test(~ TEMP_W_ES + DO, data=df[df$Well == H41 ,]) lm_o2 - lm(data=df[df$Well == H41 ,], DO ~ TEMP_W_ES) summary(lm_o2) plot(lm_o2, 1) plot(lm_o2, 2) A very quick aside and primer on multiple linear regression (univariate). Here we expect multiple continuous independent variables to effect our measured dependent variable. For example, maybe both temperature and PH affect DOC concentrations. If your goal is predictive and you have a bunch of independent variables that you would like to use to predict unmeasured dependent variables this can be a powerful method. If you are trying to explain cause effect, you need to be very careful. In general, your X variables should not be correlated. If they are, then it is difficult to parse out the effect (which one matters). If you just want to predict a value then it doenst matter, but adding highly correlated variables wont really improve your predictions. This is a huge and actively growing area of statistics (along with multivariate multiple regression) so I will just talk about how we could do simple multiple regression in R. Also, linear regression does not show causation. It might provide supportive evidence, but you need to be very careful with interpreting your data. #Lets check how our dependent variable is related to our independent variables ggplot(df, aes(x = TEMP_W_ES, y = DOC))+ geom_point() ggplot(df, aes(x = PH, y = DOC))+ geom_point() #Lets see if our independent variables are correlated cor.test(data=df, ~ PH + TEMP_W_ES) #Setting up the linear model: lm_DOC - lm(data=df, DOC ~ TEMP_W_ES*PH) #What does the * mean here? #Results from the model: summary(lm_DOC) Parting Words To wrap this up. I hope you're now convinced of how useful this free programing language is. We did not touch upon many of the incredible pacakges that have been developed. Suffice to say, I have never found a problem that I had in R that someone else hadn't already solved and posted aobut (although sometimes it takes awhile to figure out how to ask Google the right question). I know I'm sounding like a broken record, but it is always a good idea to familarize yourself with the fundamentals when you are performing some data analysis and I've founded the references at the top to be extremely useful. Also, I have had extremely good luck with stackedoverflow posts when I'm stuck or trying to figure out new methods. I encourage you to search the internet for examples of problems that you have. Start simple and refine your analysis as you go. But remember, it is best to consider how you will answer your research question before you design the experiment. It's impossible to retroactively fix a poorly designed experimental plan and from personal experience spending more time at the beginning stages will make all your future analysis much easier. Written by Will A. Overholt Oct 2018","title":"Basic statistics"},{"location":"part4/#basic-statistical-tests-with-r","text":"This is going to be fairly fast paced and brief discussion of many commonly used statistical tests and how to run them in R. I barely scratch the surface in terms of the kinds of tests available and which should be used. I HIGHLY HIGHLY recommend you check out the Handbook of Biological Statistics and the R companion book (both free online) when you are analyzing your own data!!","title":"Basic Statistical Tests with R"},{"location":"part4/#some-extremely-excellent-statistic-references","text":"Handbook of Biological Statistics http://www.biostathandbook.com/ R Companion for Biological Statistics https://rcompanion.org/rcompanion/a_02.html Intro R Statistics https://www.bioinformatics.babraham.ac.uk/training/R_Statistics/Introduction%20to%20Statistics%20with%20R%20manual.pdf Probability Statistics https://cran.r-project.org/web/packages/IPSUR/vignettes/IPSUR.pdf Handbook of Statistical Analyses https://cran.r-project.org/web/packages/HSAUR/vignettes/Ch_introduction_to_R.pdf","title":"Some extremely excellent statistic references"},{"location":"part4/#our-plan-today","text":"Today we will discuss and then explore the following ideas: - Common assumptions what these look like. - Students T tests and derivations - ANOVA and derivations - Linear Regression / Correlation","title":"Our plan today"},{"location":"part4/#assumptions-of-common-parametric-tests","text":"Independence The measurements collected are independent from each other. Common voiolations include: timeseries data repeated measurements from the same sample measurements from small spatial region Other? Normality Do the measurements come from a normal or gaussian distribution? Surprisingly, this is actually not that important since most of the common parametric tests are fairly robust to deviations from normality You can try to transform data to meet assumptions of normality A great idea is to plot the data and see what you are getting yourself into Homoscedasticity Or the assumption that different groups have the same standard deviation more of a problem if you have an unbalanced dataset again best to plot you data and get a feel for what it looks likes An example: #Loading the libraries we will be using today library(ggplot2) library(dplyr) #An example dataset ex_norm - data.frame(sampleid = seq(1,100), y = rnorm(n=100, mean=10, sd=2), treatment=rep(c( A , B ), each=50)) #Our class dataset from yesterday df - df - read.delim( simulated_dataset.txt , header=T) Lets start exploring these data! #Plotting a histogram to see how your univariate data are arranged ggplot(ex_norm, aes(y))+ geom_histogram(binwidth = 0.5) #Using a scatter plot to look at your data ggplot(ex_norm, aes(y))+ geom_point(aes(x = treatment, y = y)) #Using a box plot ggplot(ex_norm, aes(x=treatment, y=y))+ geom_boxplot(aes(y=y)) #Fancier box plot (violin plot) ggplot(ex_norm, aes(x=treatment, y=y))+ geom_violin(aes(y=y)) Working with some of our data: The joys of real life ggplot(df, aes(NO3))+ geom_histogram() ggplot(df, aes(NO3))+ geom_histogram(binwidth = 5) ggplot(df, aes(NO3))+ geom_histogram(aes(fill = Well)) ggplot(df, aes(x=Well, y = NO3))+ geom_point() ggplot(df, aes(x=Well, y = NO3))+ geom_boxplot() So what do you think? Are these groups normally distributed? We can use the Shapiro Wilk normality test to see. Just FYI, this test is quite sensitive to non-normal data. If it fails, you don't have to immediately panic since the following tests we'll talk about are much more robust to non-normal data. df % % select(Well,NO3) % % group_by(Well) % % summarise(statistic = shapiro.test(NO3)$statistic, p.value = shapiro.test(NO3)$p.value) df % % select(Well,NO3) % % group_by(Well) % % summarise(statistic = shapiro.test(sqrt(NO3))$statistic, p.value = shapiro.test(sqrt(NO3))$p.value) #This test is running the following command on each of the Wells shapiro.res - shapiro.test(df[df$Well == H43 ,]$NO3) shapiro.res$statistic shapiro.res$p.value Is the sampling scheme balanced? df % % select(Well,NO3) % %group_by(Well) % % summarise(n=n()) What can we do? #squareroot transform ggplot(df, aes(x = Well, y = sqrt(NO3)))+ geom_point()+ stat_summary(fun.y = mean , geom = point , color= blue ) #log10 transform ggplot(df, aes(x = Well, y = log10(NO3+0.5)))+ geom_point()+ stat_summary(fun.y = mean , geom = point , color= blue ) Which looks better? #Note, this test is sensitive to non-normal data as well as heteroscedastic data. #It is nice for picking the best transform though. #raw data bartlett.test(data=df, NO3 ~ Well) #squareroot transform bartlett.test(data=df, sqrt(NO3) ~ Well) #log10 transform bartlett.test(data=df, log10(NO3+1) ~ Well) OK, it is your turn. Individually or in small groups, check other variables in the data frame Ask yourself if: - normal - homoscedastic - independent - transform Make sure you look at some of the variables associated with the aquifer.","title":"Assumptions of common parametric tests:"},{"location":"part4/#students-t-tests","text":"","title":"Student's t tests"},{"location":"part4/#one-sample-t-tests","text":"These are used to compare the mean of your population to a theoretical mean #Lets see if the Nitrate from well H43 has a mean of 0. t.test(df[df$Well == H43 ,]$NO3, mu = 0, conf.level = 0.95) #We know that nitrate violates some assumptions of normality. #How does the more normal square root transformed data appear? t.test(sqrt(df[df$Well == H43 ,]$NO3), mu = 0, conf.level = 0.95) #Always a good idea to see if you stats agree with your intuition of the data ggplot(df[df$Well == H43 ,], aes(x=Well, y=NO3))+ geom_point(position = position_jitter()) #What happens with our example dataset? t.test(ex_norm$y, mu = 10, conf.level = 0.95) OK, now it is your turn, test to see if the lower aquifer (HTL) is anaerobic.","title":"One-sample t tests"},{"location":"part4/#two-sample-t-tests","text":"These are much more common (in my opinion). Here we are comparing the means between 2 groups. Lets see if the 1st half of our simulated normal data is significantly different from the last half. t.test(ex_norm[seq(1,50),]$y, ex_norm[seq(51,100),]$y) #What is this actually doing? #Lets plot our data again ggplot(df, aes(x = Aquifer, y = NO3))+ geom_boxplot() #Check for equal variances bartlett.test(data=df, NO3 ~ Aquifer) #Run the two-sample t.test based on the aquifer t.test(df[df$Aquifer == HTL ,]$NO3, df[df$Aquifer == HTU ,]$NO3) In general though, if you have a large enough sample size (approximately 5) you want to use a Welchs t test (default in R and what we've been using). This variation does not assume equal variance. In fact, many places recommend to always use this test over a students t test. If the samples have the same variance you can turn that on in this way t.test(df[df$Aquifer == HTL ,]$NO3, df[df$Aquifer == HTU ,]$NO3, var.equal = TRUE) If you do not think you samples are normally distributed you can use a Mann Whitney U test. wilcox.test(df[df$Aquifer == HTL ,]$NO3, df[df$Aquifer == HTU ,]$NO3) #Note: We get the cannot compute exact p-values since we have multiple measurements #of the same value (0) in the HTU dataset. In practice, I almost always use the default Welch's t.test for un-equal variance.","title":"Two-sample t tests"},{"location":"part4/#anova","text":"","title":"ANOVA"},{"location":"part4/#one-way-anova","text":"We can use an analysis of variance (ANOVA) test to compare the means of 2 or more groups. If it is only 2 groups, then a one-way anova and a two-sample t test are identical. There are A LOT of different ways to do ANOVA tests and we will quickly cover a few. As with everything in this section, I highly recommend you double check references before you get started. no3_aov - aov(data=df, sqrt(NO3) ~ Well) summary(no3_aov) Understanding the output: ANOVA essentially tests if the variance between your group means is different from the variance within the groups. Under the null hypothesis, all the groups have a same mean, so the variance between those group means will be the same as the average within group variation. There are 100s of books written on ANOVA and I recommend checking some of them out. Might also be good to manually calculate the values yourself once if you are interested. The ratio of these variances under the null fits an F-distribution (hence the F value). The p.value can then be determined from how different our tests F value is from the expected F value under the null hypothesis. The residuals in an R anova table refer to the within group statistics. The degrees of freedom are \"number observations\" - \"number of groups\". The group variable is specified (between-group means) and the degrees of freedom here are calculated as the \"number of groups\" - 1. Ideally you report the results as Nitrate concentrations in the wells were significantly different (one-way ANOVA, F 2,27 = 29, p 1e-7). Note, ANOVA assumes your group data are normally distributed, with equal standard deviations in the groups, and are the measurements are independent. However ANOVA is fairly robust to non-normal data and if your sampling scheme is balanced it is robust to differences in standard deviations between the groups as well. If your experimental plan is not balanced, you need to be more careful as you can get a lot more false positives (p values 0.05, but no actual differences between the means). That said, if you have a very significant p value (ours is 1.71e-7) your means are probably different. A non-parametric version of ANOVA can also be used if your data are not normal. This would be the Kruskal Wallis test. It will have a lot less power and should be picked if your data are not normal AND you dont expect them to be normal. Interestingly, some people (John McDonald of the Handbook for Biological Statistics) recommend never using this test. kruskal.test(data=df, sqrt(NO3) ~ Well) There is also a version of ANOVA if you think your data are normal, but you do not expect equal variances. oneway.test(data=df, sqrt(NO3) ~ Well) There are a couple of built in ways to check your assumptions after running the ANOVA test. #Check afterward your ANOVA for whether the groups have similar variances #Here we are plotting the residuals vs the group means. #Remember that the residuals are the data values - group mean . #There should be no relationship between the residuals and the means (flat line). #The spread should be approximately equal as well. plot(no3_aov,1) #We can also check to see if the group data are normal. plot(no3_aov, 2) #Here we are plotting quantiles of the residuals against the quantiles of a normal distribution. #Normal data will match the theoretical normal distribution giving a straight 1:1 line. OK great, now we are pretty sure that NO3 values are different between our wells. However, we want to know WHICH wells are different from each other. Remember, ANOVA only tells you if there is a diffence in your groups means and NOT which groups are different. We can do post-hoc tests to see which groups are significantly different. The most common option that I've seen is to use Tukeys HSD (honest significant differences) test. This assumes equal variances between the groups, and corrects for multiple tests. TukeyHSD(no3_aov) There are no base R functions for unbalanced / unequal variances for post-hoc testing. There are other packages you can try though (multcomp, lsmeans, DescTools, agricolae). #You can also do two-way t tests and correct the p.values for multiple tests. t.test(sqrt(df[df$Well == H52 ,]$NO3), sqrt(df[df$Well == H41 ,]$NO3))$p.value t.test(sqrt(df[df$Well == H52 ,]$NO3), sqrt(df[df$Well == H43 ,]$NO3))$p.value t.test(sqrt(df[df$Well == H43 ,]$NO3), sqrt(df[df$Well == H41 ,]$NO3))$p.value p.adjust(c(0.008654862, 0.00662454, 1.876758e-10), method= bonferroni ) Pick a different variable, and test the means. Make sure to check how badly it violates the assumptions.","title":"One-way ANOVA"},{"location":"part4/#two-way-anova","text":"You can use a two-way ANOVA when there are two parameters (factors) associated with your data. In our case, an example would be \"Well\" and \"Season\". We have sampled our 3 wells multiple times throughout the year and over multiple years. We can use a two-way ANOVA to test whether our measured data is significantly different between well, between season, and if there is a well specific seasonal effect (intereaction between our factors). Two-way ANOVA is often used for repeated measurements from the same individuals and you want to control for differences between those individuals. Two-ANOVA with replication tests 3 NULL hypotheses. 1. The means within one of our factors are the same, 2. The means within the other factor are the same. 3. There is no interaction between the factors. You can also do a two-way ANOVA test without replication (one data point per group), but you need to assume that there is no interaction term and the test is much weaker. Generally, if the interaction between your variables is significant then you need to change your analysis plan since it is not fair to test for significance in only one of the variables. Instead, you can split your data up and do 2 one-way ANOVA tests. Lets take a look for what we are testing: ggplot(df)+ geom_boxplot(aes(x = Well, y = sqrt(NO3), color = Season)) ggplot(df)+ geom_boxplot(aes(x = Season, y = sqrt(NO3), color = Well)) Thoughts? #Run the anova in R and save the result no3_2fac_aov - aov(data=df, sqrt(NO3) ~ Season + Well + Season:Well) #view information about the ANOVA summary(no3_2fac_aov) #Example of how R specifies ANOVA formulas summary(aov(data=df, sqrt(NO3) ~ Season + Well)) summary(aov(data=df, sqrt(NO3) ~ Season*Well)) Lets look at the assumptions: plot(no3_2fac_aov, 1) plot(no3_2fac_aov, 2) #Actually looks quite a bit better than I expected... #Might be a good idea to remove the outliers and re-test though. OK, so this is not really fair. But as an example (assuming no interaction was found) we can still do follow up testing between groups. TukeyHSD(no3_2fac_aov, which = Well ) Take some time and run a two-way ANOVA test with a different dependent variable.","title":"Two-way ANOVA"},{"location":"part4/#linear-regression-and-correlation","text":"Linear regression and correlation are used to investigate the relationship between two continuous variables. In general you want to know if two variables are related, how closely they are related, and mathematically describe this relationship. I am not going into details into the differences between linear regression and correlation. Check out this page for more info http://www.biostathandbook.com/linearregression.html Also, R is perfectly happy to let you run linear regression with 1 measurement variable and factors (discrete / categorical variables). In this case, it actually runs ANOVA/MANOVA in the background. I find this both convenient (you can use the same syntax and still be correct) and confusing (you may be running a different test than you expect). This is also why you see a lot of examples where people set up their ANOVA test with a linear regression model. For example: linear_model - lm(data=df, sqrt(NO3) ~ Well) summary(aov(linear_model)) Anways, lets see if Calcium and Sodium concentrations are related. ggplot(df, aes(x = Ca, y = Na))+ geom_point() This looks pretty clear, but lets test it. By default, R uses a pearson correlation test which assumes that the data are linearly related and that the residuals are normally distributed. The null hypothesis is that the variables are not related (slope = 0). cor.test(~ Ca + Na, data=df) A nonparametric correlation test commonly used is the Spearman rank correlation test. It does not assume a distribution for the variables of if they are related. cor.test(~ Ca + Na, data=df, method= spearman ) The correlation coefficient and the p value tend to be reported. The correlation coefficient (rho) goes from -1 to 1, where -1 indicates the variables are perfectly negatively correlated, and 1 indicates they are perfectly positively correlated. The p value is calculated using a t distribution with n-2 degrees of freedom. If we want to generate a linear regression model for these measurements, we can use the built in function lm(). lm(Ca ~ Na, data = df) The formula format R uses is Y ~ X1 The \"Y\" variable is traditionally assigned as the dependent variable while the \"X\" variable(s) are the independent ones. Our example is not great since we are looking at two dependent variables that are probably not independent. In R the lm() function fits the ordinary-least-squares regression line, or the line that minimizes the distance between your observations and the line itself. This works best for analysing two continuous variables with both independent and dependent variables. It is not valid for 2 dependent variables (assuming there is no cause and effect relationship). There are other techniques in this instance, however, they are not valid if you want to predict unmeasured values. Anyways, I strongly recommend you read the literature or use methods typically employed in your field before running these analyses on your own data. But for us, lets assume everything is fine and explore our linear regression results. linear_model1 - lm(Ca ~ Na, data = df) summary(linear_model1) #Note that the pvalue the t.value for the slope are the same as we got from the Pearson correlation analysis. cor.test(~ Ca + Na, data=df) The r^2 value (coefficient of determination) is a measure of how well your regression line fits the data, or how much of the variance of your Y variable (dependent) is explained by the X variable. Values close to 1 indicate your observed Y values fall on your regression line while values close to 0 indicate there is no relationship between your variables. Like with ANOVA, we can explore data characteristics from the linear regression results. #Check for homoscedascity bias (line should be flat around 0) plot(linear_model1,1) #Check for normality plot(linear_model1,2) OK, lets try another example. We know that oxygen concentrations are physically dependent on temperature. Lets see if there is a relationship between these values. Which is the dependent and which is the independent variable? ggplot(df, aes(x = TEMP_W_ES, y=DO))+ geom_point() cor.test(~ TEMP_W_ES + DO, data=df) summary(lm(DO ~ TEMP_W_ES, data=df)) But, wait. Lets think about this? Does a value of 0 depend on temperature in this case? ggplot(df, aes(x = TEMP_W_ES, y=DO))+ geom_point(aes(color=Well)) #Lets look at only wells with oxygen. cor.test(~ TEMP_W_ES + DO, data=df[df$Well == H41 ,]) lm_o2 - lm(data=df[df$Well == H41 ,], DO ~ TEMP_W_ES) summary(lm_o2) plot(lm_o2, 1) plot(lm_o2, 2) A very quick aside and primer on multiple linear regression (univariate). Here we expect multiple continuous independent variables to effect our measured dependent variable. For example, maybe both temperature and PH affect DOC concentrations. If your goal is predictive and you have a bunch of independent variables that you would like to use to predict unmeasured dependent variables this can be a powerful method. If you are trying to explain cause effect, you need to be very careful. In general, your X variables should not be correlated. If they are, then it is difficult to parse out the effect (which one matters). If you just want to predict a value then it doenst matter, but adding highly correlated variables wont really improve your predictions. This is a huge and actively growing area of statistics (along with multivariate multiple regression) so I will just talk about how we could do simple multiple regression in R. Also, linear regression does not show causation. It might provide supportive evidence, but you need to be very careful with interpreting your data. #Lets check how our dependent variable is related to our independent variables ggplot(df, aes(x = TEMP_W_ES, y = DOC))+ geom_point() ggplot(df, aes(x = PH, y = DOC))+ geom_point() #Lets see if our independent variables are correlated cor.test(data=df, ~ PH + TEMP_W_ES) #Setting up the linear model: lm_DOC - lm(data=df, DOC ~ TEMP_W_ES*PH) #What does the * mean here? #Results from the model: summary(lm_DOC)","title":"Linear Regression and Correlation"},{"location":"part4/#parting-words","text":"To wrap this up. I hope you're now convinced of how useful this free programing language is. We did not touch upon many of the incredible pacakges that have been developed. Suffice to say, I have never found a problem that I had in R that someone else hadn't already solved and posted aobut (although sometimes it takes awhile to figure out how to ask Google the right question). I know I'm sounding like a broken record, but it is always a good idea to familarize yourself with the fundamentals when you are performing some data analysis and I've founded the references at the top to be extremely useful. Also, I have had extremely good luck with stackedoverflow posts when I'm stuck or trying to figure out new methods. I encourage you to search the internet for examples of problems that you have. Start simple and refine your analysis as you go. But remember, it is best to consider how you will answer your research question before you design the experiment. It's impossible to retroactively fix a poorly designed experimental plan and from personal experience spending more time at the beginning stages will make all your future analysis much easier. Written by Will A. Overholt Oct 2018","title":"Parting Words"},{"location":"part5/","text":"Multivariate Analyses of Microbial Communities with R Importing multivariate data using phyloseq Loading the required packages We recommend checking out some of the following references: GUSTA ME Phyloseq Homepage Ecological Analysis of Ecological Communities First we'll clear our R environment of all attached objects and define the memory limit for windows systems. # Memory limit only needed if you use windows system #clear all currently attached R objects rm(list=ls()) memory.limit() memory.limit(size=56000) Install the following packages. You will only need to do this once on your computer. source( https://bioconductor.org/biocLite.R ) biocLite('phyloseq') install.packages(vegan) biocLite( Biostrings ) install.packages( biostrings ) install.packages( reshape2 ) install.packages( ape ) install.packages( picante ) install.packages( ggpubr ) install.packages( gridExtra ) install.packages( ggplot2 ) install.packages( devtools ) install_github( vqv/ggbiplot ) install_github( pmartinezarbizu/pairwiseAdonis/pairwiseAdonis ) Then load the packages we need: library(phyloseq) library(vegan) ibrary(Biostrings) # To read fasta file library(reshape2) library(ape) # to read tree file library(scales) library(picante) library(ggplot2) library(ggpubr) library(gridExtra) library(ggbiplot) library(pairwiseAdonis) #For reproducibility set.seed(123) Importing our data Check out this link for the authors guide on importing data into phyloseq. #First set our working directory (this will be where you placed the data files setwd( C:/IF/microbial community course ) ###### Read data tables otu_tb - read.table( data/otu_table.txt , sep= \\t , row.names=1,header=T) tax_tb - read.table( data/tax_table.txt , sep= \\t , row.names=1,header=T) metadata_tb - read.csv( data/simulated_dataset.txt , sep= \\t , dec = . , row.names=1, header=TRUE) tree - read.tree( data/rep_otu.phylip.tre ) refseq - readDNAStringSet( data/rep_otu.fasta ) # import into phyloseq object OTU - otu_table(as.matrix(otu_tb), taxa_are_rows = TRUE) TAX - tax_table(as.matrix(tax_tb)) METADATA - sample_data(metadata_tb) TREE - phy_tree(tree) student_data - phyloseq(OTU, TAX, METADATA, TREE, refseq) save(student_data, file = student_data.phyloseq ) # Check the phyloseq file student_data sample_names(student_data) head(otu_table(student_data)) head(tax_table(student_data)) str(sample_data(student_data)) # check internal structure of the metadata file Generating intial community composition figures to explore our data First off we'll create a directory to save our figures and datasets. dir.create( output ) # create a new directory Data preprocessing steps We'll use these objects for the rest of the analyses Removing singletons For these examples we'll remove sequences that only occur once in the dataset since we cannot be sure they are biological units or caused by sequencing errors. student_data_rs - prune_taxa(taxa_sums(student_data) 1, student_data) For the next plots we'll scale the species counts into a percent abundance value. We'll also use this dataframe in other places. student_data_prop - transform_sample_counts(student_data_rs, function(x) 100*x/sum(x)) The second is to rarefy our data by subsampling to an even depth. Note there are more robust methods to handle uneven datasets and we recommend checking out this publication by McMurdie and Holmes data_rarefy - rarefy_even_depth(student_data_rs, sample.size = min(sample_sums(student_data_rs)), verbose = FALSE, replace = TRUE) Generating a barplot of the dominant taxonomic groups To start off with we will merge all OTUs per well and the summarize them by the phylum level student_data_prop_merged - merge_samples(student_data_prop, Well ) We need to re-apply names, before after merging, phyloseq will change the wells into level 1, 2, 3, not H41, H43 and H52 as before sample_data(student_data_prop_merged)$Well - c( H41 , H43 , H52 ) Then averge your data again (value should be 100% per sample) student_data_prop_merged_prop - transform_sample_counts(student_data_prop_merged, function(x) 100*x/sum(x)) Merges species that have the same taxonomy at a certain taxaonomic rank. RANK= Phylum # select Phylum as rank student_data_prop_merged_prop_rank - tax_glom(student_data_prop_merged_prop, taxrank=RANK) Generate the barplot using the phyolseq function \"plot_bar\" plot_bar(student_data_prop_merged_prop_rank, fill=RANK) Subset and show the composition of the Actinobacteria We'll subset our dataset to grab only bacterial OTU from the phylum Actinobacteria actino - subset_taxa(student_data_prop_merged_prop, Phylum == Actinobacteria ) We'll define our own vector of colors to plot by: set_colors - c( #DA5724 , #508578 , #c481fd , #CD9BCD , #AD6F3B , #652926 , #6dcff6 , #C84248 , #1e90ff , #8569D5 , #cc0953 , #D1A33D , grey , pink , gold , #8A7C64 , #599861 , navy , #5F7FC7 , tomato , #673770 , #008080 , #2F4F4F , #FAEBD7 , #ff1493 , #5e738f , #808000 , #D14285 , #ffa500 , cbd588 , wheat , #d2b48c , cyan2 , black , #BC8F8F , #800000 , #008B8B , #BC8F8F ) Then we can generate the following bar plot: plot_bar(actino , x= Class , fill= Order , facet_grid=~Well) + ylab( Relative abundance (%) ) + scale_fill_manual(values=set_colors) Improving our first bar plot We'll extract the abundance data for the abundance phyla (e.g. Relative abundance 1%) and save the data student_data_prop_merged_prop_rank_filter - filter_taxa(student_data_prop_merged_prop_rank, function(x) mean(x) 1, TRUE) abun.phylum - psmelt(student_data_prop_merged_prop_rank_filter) View(abun.phylum) If everything looks good, we can save this dataset as a text file write.table(abun.phylum, output/phylum_over1percent.txt , sep= \\t , dec= . , col.names=NA) Then we'll calculate the sum of all remaining groups for an \"Others\" category. others - tax_glom(student_data_prop_merged_prop_rank_filter, Kingdom ) otu_table(others) - 100 - otu_table(others) tax_table(others)@.Data[,2:6] - Others # Define all taxanomic levels as Others taxa_names(others) - Others # Define the taxa name as Others Next we can combine the abundant taxa with the sum of rare taxa OTU1 - otu_table(cbind(otu_table(others), otu_table(student_data_prop_merged_prop_rank_filter) ), taxa_are_rows = FALSE) TAX1 - tax_table(rbind(tax_table(others), tax_table(student_data_prop_merged_prop_rank_filter) )) METADATA1 - sample_data(student_data_prop_merged_prop_rank_filter) final - phyloseq(OTU1, TAX1, METADATA1) Plot the abundance phyla using bar or heatmap plot_bar(final, fill=RANK) + scale_fill_manual(values=set_colors)+ theme(axis.text.x = element_text(angle = 0, vjust = 0.5, hjust = 0.5)) And we can save the figure using the ggplot function, \"ggsave\" ggsave( output/taxonomic composition_phylum.png , width = 4, height = 4) We could also generate a heatmap of the same data plot_heatmap(final, method = PCoA , distance = bray , sample.label= Well , taxa.label= Phylum , low= #000033 , high= #FF3300 )+ theme(axis.text.x = element_text(angle = 0, vjust = 0.5, hjust = 0.5)) # Note: by default plot_heatmap takes a log transformation (trans = log_trans(4)) ggsave( output/heatmap_phylum.png , width = 4, height = 4) Extracting the top 30 OTUs and showing the abundance across samples top30 - prune_taxa(names(sort(taxa_sums(student_data_prop),TRUE)[1:30]), student_data_prop) plot_heatmap(top30, # method = NMDS , # default method method = NULL, # distance = bray , sample.label= Well , taxa.label= Class , trans = NULL)+ theme(axis.text.x = element_text(angle = -90, vjust = 0.5, hjust = 0.5)) ggsave( output/heatmap_top30.png , width = 4, height = 4) Phylogenetic tree We can use \"Ape\" and the provided phylogenetic tree to plot our abundant OTUs top20 - prune_taxa(names(sort(taxa_sums(student_data_prop_merged_prop),TRUE)[1:20]), student_data_prop_merged_prop) plot_tree(top20, color = Phylum , shape = Well , #label.tips = Family , # size = abundance , plot.margin = 0.5, ladderize = TRUE) + # coord_polar(theta= y )+ scale_shape_manual(values = c(1,2,16)) ggsave( output/phylo_tree_phylum.png , width = 4, height =4) Exploring Alpha Diversity Many richness estimates are modeled on singletons and doubletons in the abundance data. You need to leave them in the dataset if you want a meaningful estimate. We'll be using the rarefied dataset we created in the \"Data Preprocessing steps\" above. student_data_rarefy - data_rarefy plot_richness(student_data_rarefy) # warnings probably because of the graphics that didn't show clearly p_alpha - plot_richness(student_data_rarefy, x= Well , measures=c( Simpson , Shannon ), color= Well ) + geom_point(size=5, alpha=0.7) + scale_color_manual(values = c( red , blue , black )) p_alpha estimate alpha diversity alphadiv - estimate_richness(student_data_rarefy # , measures=c( Simpson , Shannon ) ) alphadiv Phylogenetic Diversity Start off with a function that transposes the OTU matrix into a form that vegan can use. vegan_otu - function(physeq) { OTU - otu_table(physeq) if (taxa_are_rows(OTU)) { OTU - t(OTU) } return(as(OTU, matrix )) } To calculate Faith's phylogenetic distance we need a community data matrix and a phylo tree object FaithPD - pd(vegan_otu(student_data_rarefy), phy_tree(student_data_rarefy)) ## Returns a dataframe of the PD and species richness (SR) values for all samples alphadiv2 - data.frame(alphadiv, FaithPD) write.table(alphadiv2, output/alpha diversity.txt , sep= \\t , dec= . , col.names=NA) # Note: SR equals Observed using different functions identical(alphadiv2$SR, alphadiv2$Observed) Now you can go on with univariate data analysis, e.g. ANOVA or plot nice figures, For instance: # combine the diversity results with the sample data file sampledf - data.frame(sample_data(student_data_rarefy)) df - data.frame(alphadiv2, sampledf) Plot boxplot with significance Add significance levels of the diversity indices between different wells my_comparisons - list( c( H41 , H43 ), c( H43 , H52 ), c( H41 , H52 ) ) Phylogenetic diversity pd_well - ggboxplot(df, Well , PD , color = Well , palette =c( red , blue , black ), add = jitter , shape = Aquifer )+ stat_compare_means(comparisons = my_comparisons, label = p.signif ) + # pairwise comparison stat_compare_means(label.y = 50) # Add global p-value Shannon diversity shannon_well - ggboxplot(df, Well , Shannon , color = Well , palette =c( red , blue , black ), add = jitter , shape = Aquifer )+ stat_compare_means(comparisons = my_comparisons, label = p.signif ) + # pairwise comparison stat_compare_means(label.y = 5.5) # Add global p-value Plotting both together: grid.arrange(pd_well, shannon_well, ncol=2) dev.copy(png, file= alpha_diversity.png , width=1500, height=600, res=144) dev.off() #could also use ggsave() here Principal Component Analysis of Geochemical Variables Read the data tables metadata_tb - read.csv( data/simulated_dataset.txt , sep= \\t , dec = . , row.names=1, header=TRUE) str(metadata_tb) # check internal structure of the metadata file metadata_tb.pca - prcomp(metadata_tb[,c(7:21)], center = TRUE, scale. = TRUE) # center = TRUE: a logical value indicating whether the variables should be shifted to be zero centered # center = TRUE, scale. = TRUE: the variables is adviced to be scaled to have unit variance before the analysis takes place To check the results of the PCA. The print method returns the standard deviation of each of the four PCs, and their rotation (or loadings), which are the coefficients of the linear combinations of the continuous variables. print(metadata_tb.pca) The summary method describe the importance of the PCs. We can see there that the first two PCs accounts for more than 51% of the variance of the data. summary(metadata_tb.pca) Plotting with base R The plot method returns a plot of the variances (y-axis) associated with the PCs (x-axis). The Figure below is useful to decide how many PCs to retain for further analysis. We can see that the first two PCs explain most of the variability in the data. plot(metadata_tb.pca, type = l ) plot(metadata_tb.pca) A simple PCA plot biplot(metadata_tb.pca, choices = 1:2, scale = 1) Using ggbiplot ggbiplot(metadata_tb.pca) Nice looking plot ggbiplot(metadata_tb.pca, ellipse=TRUE, choices = 1:2, var.axes = TRUE, # Draw arrows for the variables? obs.scale = 1, var.scale = 1, labels=rownames(metadata_tb), groups=metadata_tb$Well) + scale_colour_manual(name= Well , values= c( red , blue , black ))+ ggtitle( PCA of simulated dataset )+ theme_bw()+ theme(legend.position = right ) ggsave( output/PCA of simulated dataset.png , width = 8, height = 6) Beta Diversity Ordination Plots Clustering and Dendrogram Calculate the distance matrix between samples dis - phyloseq::distance(student_data_prop, bray ) # distance calculation Hierarchical clustering hc1 - hclust(dis, method= average ) hc2 - hclust(dis, method= ward.D ) par(mfrow = c(2,1)) plot(hc1) plot(hc2) Add colors to the tips sampledf - data.frame(sample_data(student_data_prop)) well - sampledf$Well # palette -hue_pal()(length(levels(well))) palette - c( red , blue , black ) tipColoor - col_factor(palette, levels=levels(well))(well) Save it as phylo object clust.uf -as.phylo(hclust(dis,method= ward.D2 )) # png( output/cluster.png ) plot(clust.uf,tip.color=tipColoor, direction= downwards ) #10*20 dev.off() More info on different clustering methods: http://girke.bioinformatics.ucr.edu/GEN242/pages/mydoc/Rclustering.html Unconstrained ordination Perform an ordination on phyloseq data set.seed(123) NMDS plot based on Bray-Curtis distance ord - ordinate(student_data_prop, NMDS , bray ) plot ordination p = plot_ordination(student_data_prop, ord, # type= biplot , type= split , # type= samples , # type= taxa , shape= Well , color= Phylum , label= Well , title= NMDS based on Bray-Curtis distance )+ # geom_point(size=7, alpha=0.75)+ theme_bw() print(p) ggsave( output/NMDS_bray.png , width = 8, height = 4) PCoA plot based on Unifrac distance ordu = ordinate(student_data_prop, PCoA , unifrac , weighted=TRUE) summary(ordu) p1 = plot_ordination(student_data_prop, ordu, axes = 1:2, type= samples , shape= Season , color= Well , #label= Well , # type= taxa ,color= Phylum , title= PCoA based on weighted UniFrac distance )+ geom_point(size=7, alpha=0.75)+ scale_colour_manual(values= c( red , blue , black ))+ scale_shape_manual( values= c(0, 1,16,17))+ theme_bw() print(p1) remove the inner dots p1$layers -p1$layer [-1] print(p1) ggsave( output/PCoA_wunifrac.png , width = 8, height = 6) Scree plot: shows the fraction of total variance in the data as explained or represented by each PC. p_scree - plot_ordination(student_data_prop, ordu, type= scree ) print(p_scree) Constrained ordination RDA ordu2 = ordinate(student_data_prop, RDA , unifrac , weighted=TRUE) summary(ordu2) p2 = plot_ordination(student_data_prop, ordu2, type= samples , # shape= Season , color= Well , #label= Well , # type= taxa ,color= Phylum , title= RDA based on weighted UniFrac distance )+ geom_point(size=7, alpha=0.75)+ theme_bw() print(p2) Remove the inner dots p2$layers -p2$layer [-1] print(p2) ggsave( output/RDA_wunifrac.png , width = 6, height = 4) Scree plot: shows the fraction of total variance in the data as explained or represented by each PC. p2_scree - plot_ordination(student_data_prop, ordu2, type= scree ) print(p2_scree) ccA plot based on Unifrac distance set.seed(123) ordu3 = ordinate(student_data_prop, CCA , unifrac , weighted=TRUE) summary(ordu3) p3 = plot_ordination(student_data_prop, ordu3, axes = 1:2, type= samples , shape= Well , color= Well , #label= Well , # type= taxa ,color= Phylum , title= CCA based on weighted UniFrac distance )+ geom_point(size=7, alpha=0.75)+ theme_bw() print(p3) # remove the inner dots p3$layers -p3$layer [-1] print(p3) # scree plot: shows the fraction of total variance in the data as explained or represented by each PC. p3_scree - plot_ordination(student_data_prop, ordu3, type= scree ) print(p3_scree) Beta Diversity Hypothesis Testing PERMANOVA using adonis function in Vegan package First calculate bray curtis distance matrix using either weighted unifrac distance or Bray-Curtis distance # student_data_prop_bray - phyloseq::distance(student_data_prop, method = unifrac , weighted=TRUE) student_data_prop_bray - phyloseq::distance(student_data_prop, method = bray ) Make a data frame from the sample_data sampledf - data.frame(sample_data(student_data_prop)) head(sampledf) Then run the Adonis test (Number of permutations: 999) ht_well - adonis(student_data_prop_bray ~ Well, data = sampledf) # *** This output tells us that our adonis test is significant so we can reject the null hypothesis that our three countries have the same centroid. ht_well ht_well$aov.tab$ Pr( F) write.table(data.frame(ht_well$aov.tab), output/permanova_well.txt , sep= \\t , dec= . , col.names=NA) Posthoc Pairwise Adonis This is an R wrapper function for multilevel pairwise comparison using adonis (~Permanova) from package 'vegan'. The function returns adjusted p-values using p.adjust(). First, we need to convert the phyloseq object into an OTU-table vegan_otu - function(physeq) { OTU - otu_table(physeq) if (taxa_are_rows(OTU)) { OTU - t(OTU) } return(as(OTU, matrix )) } pairwise.adonis(vegan_otu(student_data_prop), sampledf$Well) !!! Your Turn Test if there was any effect of other factors on bacterial community composition, e.g. Season Hint: adonis(student_data_prop_bray ~ Season, data = sampledf) # This output tells us that our adonis test is non-significant You can do the rest.... Analysis of Similarities using anosim function in Vegan package ano_well - anosim(student_data_prop_bray, sampledf$Well) summary(ano_well) par(mfrow=c(1,1)) plot(ano_well) You can test if there was any effect of other factors on bacterial community composition, e.g. Season ano_season - anosim(student_data_prop_bray, sampledf$Season) Homogeneity of dispersion test using betadisper function in Vegan package This is a nonmetric test based on permutations dis_well - betadisper(student_data_prop_bray, sampledf$Well) permutest(dis_well) # ** dis_season - betadisper(student_data_prop_bray, sampledf$Season) permutest(dis_season) # ns Additionally, our betadisper results are significant, meaning we cannot reject the null hypothesis that our groups have the same dispersions. This means we cannot be confident that our adonis result is a real result, and not due to differences in group dispersions Tukey's Honest Significant Differences well.HSD - TukeyHSD(dis_well) well.HSD Plot dispersion distances between groups par(mfrow=c(1,2)) plot(well.HSD, las=1) plot(dis_well, las=1) Linking to environmental parameters and biplots We'll use OTUs with mean relative abundance over 0.01% for CCA analysis student_data_prop_filter - filter_taxa(student_data_prop, function(x) mean(x) 0.01, TRUE) Import the OTU table and geochemical data: otutable - vegan_otu(student_data_prop_filter) sampledf - data.frame(sample_data(student_data_prop_filter)) To check whether to choose CCA or RDA, we should use Detrended correspondence analysis (DCA) dca - decorana(otutable) dca Check the \"Axis lengths\" of the DCA1 (Important) summary(dca) # to check the Axis lengths of the DCA1 (Important) If this value 3, it is better to use RDA If this value 4, it is better to use CCA If this value is between 3 and 4, either use CCA or RDA Envfit + CCA/RDA Much more powerful when more complex factors are tested CCA Scale species to unit variance df - data.frame(scale(sampledf[,-c(1:6)])) Test all environmental factors ord_all - cca(otutable ~ ., data=df) You can choose to test only some of the environmental factors ord_select - cca(otutable ~ EC + DO + PH, df) # RDA bray - vegdist(otutable, bray ) ord_all - capscale(bray~., df]) Plot all environmental factors plot(ord_all, type = p , scaling = sites ) Variance Inflation Factors Linear dependencies between constraints can be investigated via the variance inflation factor or VIF VIF is a measure of how much the variance of $\\hat{\\beta}_j$ is inflated by presence of other covariates Lots of rules of thumb VIF = 20 indicates strong collinearity in constraints VIF = 10 potnetially of concern should be looked at They will be completely removed from the estimation, and no biplot scores or centroids are calculated for these aliased constraints. vif.cca(ord) temp - vif.cca(ord_all) temp select_para - names(temp[temp 10]) select_para Keep the environmental factors with VIF less than 10 ord - cca(otutable ~ ., df[,select_para]) Fit environmental vectors onto the ordination The function fits environmental vectors or factors onto an ordination. The projections of points onto vectors have maximum correlation with corresponding environmental variables, and the factors show the averages of factor levels. fit - envfit(ord, df[,select_para], perm = 999, display = lc , scaling = sites ) fit$vectors # check the significance Extract the best (significant) variables (p 0.05) spp.scrs - as.data.frame(scores(fit, display = vectors )) spp.scrs pval - fit$vectors$pvals pval Data for the envfit arrows This is necessary, see Gavin Simpson http://stackoverflow.com/questions/14711470/plotting-envfit-vectors-vegan-package-in-ggplot2/25425258#25425258 fdat - cbind(spp.scrs, Vector = rownames(spp.scrs), pval) Now select only the signifant factors bestEnvVariables -rownames(fdat)[fdat$pval =0.05] If you have NA entries, remove them # bestEnvVariables -bestEnvVariables[!is.na(bestEnvVariables)] Redo CCA using the best environmental variables eval(parse(text=paste( ord1 - cca(otutable ~ ,do.call(paste,c(as.list(bestEnvVariables),sep= + )), ,data=sampledf) ,sep= ))) summary(ord1) You can also use anova.cca to select the significant variables anova.cca(ord1, perm=9999) anova.cca(ord1, by= margin , perm=9999) # marginal effects of the terms anova.cca(ord1, by= terms , perm=9999) # sequential anova.cca(ord1, by= axis ) # axis, slow drop1(ord1, test= perm ) Now re-fit the environmental factors on ordination fit1 - envfit(ord1,sampledf[,bestEnvVariables], perm = 999, display = lc , scaling = sites ) Simple triplot plot(ord1) Simple biplot plot(ord1, type= n ) Plot the samples points(ord1, display = sites , col = as.numeric(Moisture), pch=16) Choose the significant environmental factors plot(fit1, col = red , cex=1.2, axis=TRUE, p.max = 0.05) summary(ord1) Plot using ggplot2 spp.scrs - data.frame(scores(fit1, display = vectors )) pval - fit1$vectors$pvals #data for the envfit arrows spp.scrs - cbind(spp.scrs, Vector = rownames(spp.scrs), pval) # vector table scrs - as.data.frame(scores(ord1, display = sites )) # sample table scrs - cbind(scrs, sampledf) spp.scrs1 - subset(spp.scrs, pval =0.05) #extracts relevant environment vectors from envifit Biplot library(digest) p - ggplot(scrs) + geom_point(mapping = aes(x = CCA1, y = CCA2, colour = Well), alpha = 0.8, size = 8) + #coord_fixed() + ## need aspect ratio of 1! geom_segment(data = spp.scrs1, aes(x = 0, xend = CCA1*2.5, y = 0, yend = CCA2*2.5), arrow = arrow(length = unit(0.25, cm )), colour = blue , size=1) + geom_text(data = spp.scrs1, aes(x = CCA1*3, y = CCA2*3, label = Vector), size = 4) + theme_bw() + theme(axis.title=element_text(size=14), axis.text=element_text(size=14), legend.text=element_text(size=14)) + #scale_color_manual(values = set_colors3, guide = guide_legend(ncol=1)) + scale_color_manual(values =c( red , blue , black ), guide = guide_legend(ncol=1)) + #xlim(-2,3)+ theme(legend.position= right ) p ggsave( output/cca_linking_env.png , width = 6, height = 4) Question ggplot: Triplot? Hint:extract species scores spe - scores(ord1, display = species ) tax - tax_table(student_data_prop_filter) otu - otu_table(student_data_prop_filter) abundance - rowMeans(x=otu)# calculate mean abundance of each OTU df2 - data.frame(spe, tax, Abundance = abundance) df2 - subset(df2, abundance =0.05) More info on model selection: https://github.com/naupaka/esa_vegan/blob/master/03-constrained-ordination/constrained-ordination.md Bioindicators biocLite( DESeq2 ) biocLite( IRanges ) library(IRanges) library(DESeq2) library(ggpubr) For this analysis, we use the count data. because this method does its own normalization If we only want to identy the taxa that discriminate H43 and H52 student_data_well - subset_samples(student_data, Well!= H41 ) DESeq2 The following two lines actually do all the complicated DESeq2 work. The function phyloseq_to_deseq2 converts your phyloseq-format microbiome data into a DESeqDataSet with dispersions estimated using the experimental design formula, also shown (the ~Well term). dds = phyloseq_to_deseq2(student_data_well, ~ Well) Then DESeq function does the rest of the testing - estimation of size factors: estimateSizeFactors - estimation of dispersion: estimateDispersions - Negative Binomial GLM fitting and Wald statistics: nbinomWaldTest Wald statistics: nbinomWaldTest dds - DESeq(dds, test= Wald , fitType= parametric ) Negative Binomial GLM fitting and ddsLRT - DESeq(dds, test= LRT , reduced= ~ 1) Check our results res = results(dds, cooksCutoff = FALSE) alpha = 0.001 sigtab = res[which(res$padj alpha), ] # sigtab = sigtab[which(abs(sigtab$log2FoldChange) 2), ] # sometimes you want to report the taxa that had higher log2FoldChange than 2 sigtab = cbind(as(sigtab, data.frame ), as(tax_table(student_data_well)[rownames(sigtab), ], matrix )) head(sigtab) Plot Phylum order x = tapply(sigtab$log2FoldChange, sigtab$Phylum, function(x) max(x)) x = sort(x, TRUE) sigtab$Phylum = factor(as.character(sigtab$Phylum), levels=names(x)) Genus order x = tapply(sigtab$log2FoldChange, sigtab$Genus, function(x) max(x)) x = sort(x, TRUE) sigtab$Genus = factor(as.character(sigtab$Genus), levels=names(x)) Ploting (Note: H43 is used as a control, FoldChange = H52/H43) sigtab$OTU - rownames(sigtab) ggbarplot(sigtab, x = OTU , y = log2FoldChange , fill = Phylum , # change fill color by mpg_level color = white , # Set bar border colors to white palette = jco , # jco journal color palett. see ?ggpar sort.val = asc , # Sort the value in ascending order sort.by.groups = FALSE, # Don't sort inside each group ylab = log2FoldChange , legend.title = Phylum , rotate = TRUE, ggtheme = theme_classic() ) ggsave( output/bioindicators.png , width = 6, height = 3) Show the abundance difference of the interesting taxa between the two wells select - rownames(sigtab) student_data_well_select - subset_taxa(student_data_well, rownames(tax_table(student_data_well)) %in% select[2]) plot_bar(student_data_well_select, Well , fill= Genus ) + ylab( Sum of normalized abundance ) + theme(axis.text.x = element_text(angle = -90, vjust = 0, hjust = 1)) ggsave( output/bioindicators_bar.png , width = 6, height = 3) plot_heatmap(student_data_well_select, sample.label= Well , taxa.label= Genus , taxa.order= Phylum , sample.order= Well ) Random Forest https://rstudio-pubs-static.s3.amazonaws.com/115631_7397b7cf67534479ae80f70546610eea.html Written by Lijuan Yan Oct 2018","title":"Multivariate analysis"},{"location":"part5/#multivariate-analyses-of-microbial-communities-with-r","text":"","title":"Multivariate Analyses of Microbial Communities with R"},{"location":"part5/#importing-multivariate-data-using-phyloseq","text":"Loading the required packages We recommend checking out some of the following references: GUSTA ME Phyloseq Homepage Ecological Analysis of Ecological Communities First we'll clear our R environment of all attached objects and define the memory limit for windows systems. # Memory limit only needed if you use windows system #clear all currently attached R objects rm(list=ls()) memory.limit() memory.limit(size=56000) Install the following packages. You will only need to do this once on your computer. source( https://bioconductor.org/biocLite.R ) biocLite('phyloseq') install.packages(vegan) biocLite( Biostrings ) install.packages( biostrings ) install.packages( reshape2 ) install.packages( ape ) install.packages( picante ) install.packages( ggpubr ) install.packages( gridExtra ) install.packages( ggplot2 ) install.packages( devtools ) install_github( vqv/ggbiplot ) install_github( pmartinezarbizu/pairwiseAdonis/pairwiseAdonis ) Then load the packages we need: library(phyloseq) library(vegan) ibrary(Biostrings) # To read fasta file library(reshape2) library(ape) # to read tree file library(scales) library(picante) library(ggplot2) library(ggpubr) library(gridExtra) library(ggbiplot) library(pairwiseAdonis) #For reproducibility set.seed(123)","title":"Importing multivariate data using phyloseq"},{"location":"part5/#importing-our-data","text":"Check out this link for the authors guide on importing data into phyloseq. #First set our working directory (this will be where you placed the data files setwd( C:/IF/microbial community course ) ###### Read data tables otu_tb - read.table( data/otu_table.txt , sep= \\t , row.names=1,header=T) tax_tb - read.table( data/tax_table.txt , sep= \\t , row.names=1,header=T) metadata_tb - read.csv( data/simulated_dataset.txt , sep= \\t , dec = . , row.names=1, header=TRUE) tree - read.tree( data/rep_otu.phylip.tre ) refseq - readDNAStringSet( data/rep_otu.fasta ) # import into phyloseq object OTU - otu_table(as.matrix(otu_tb), taxa_are_rows = TRUE) TAX - tax_table(as.matrix(tax_tb)) METADATA - sample_data(metadata_tb) TREE - phy_tree(tree) student_data - phyloseq(OTU, TAX, METADATA, TREE, refseq) save(student_data, file = student_data.phyloseq ) # Check the phyloseq file student_data sample_names(student_data) head(otu_table(student_data)) head(tax_table(student_data)) str(sample_data(student_data)) # check internal structure of the metadata file","title":"Importing our data"},{"location":"part5/#generating-intial-community-composition-figures-to-explore-our-data","text":"First off we'll create a directory to save our figures and datasets. dir.create( output ) # create a new directory","title":"Generating intial community composition figures to explore our data"},{"location":"part5/#data-preprocessing-steps","text":"We'll use these objects for the rest of the analyses","title":"Data preprocessing steps"},{"location":"part5/#removing-singletons","text":"For these examples we'll remove sequences that only occur once in the dataset since we cannot be sure they are biological units or caused by sequencing errors. student_data_rs - prune_taxa(taxa_sums(student_data) 1, student_data) For the next plots we'll scale the species counts into a percent abundance value. We'll also use this dataframe in other places. student_data_prop - transform_sample_counts(student_data_rs, function(x) 100*x/sum(x)) The second is to rarefy our data by subsampling to an even depth. Note there are more robust methods to handle uneven datasets and we recommend checking out this publication by McMurdie and Holmes data_rarefy - rarefy_even_depth(student_data_rs, sample.size = min(sample_sums(student_data_rs)), verbose = FALSE, replace = TRUE)","title":"Removing singletons"},{"location":"part5/#generating-a-barplot-of-the-dominant-taxonomic-groups","text":"To start off with we will merge all OTUs per well and the summarize them by the phylum level student_data_prop_merged - merge_samples(student_data_prop, Well ) We need to re-apply names, before after merging, phyloseq will change the wells into level 1, 2, 3, not H41, H43 and H52 as before sample_data(student_data_prop_merged)$Well - c( H41 , H43 , H52 ) Then averge your data again (value should be 100% per sample) student_data_prop_merged_prop - transform_sample_counts(student_data_prop_merged, function(x) 100*x/sum(x)) Merges species that have the same taxonomy at a certain taxaonomic rank. RANK= Phylum # select Phylum as rank student_data_prop_merged_prop_rank - tax_glom(student_data_prop_merged_prop, taxrank=RANK) Generate the barplot using the phyolseq function \"plot_bar\" plot_bar(student_data_prop_merged_prop_rank, fill=RANK)","title":"Generating a barplot of the dominant taxonomic groups"},{"location":"part5/#subset-and-show-the-composition-of-the-actinobacteria","text":"We'll subset our dataset to grab only bacterial OTU from the phylum Actinobacteria actino - subset_taxa(student_data_prop_merged_prop, Phylum == Actinobacteria ) We'll define our own vector of colors to plot by: set_colors - c( #DA5724 , #508578 , #c481fd , #CD9BCD , #AD6F3B , #652926 , #6dcff6 , #C84248 , #1e90ff , #8569D5 , #cc0953 , #D1A33D , grey , pink , gold , #8A7C64 , #599861 , navy , #5F7FC7 , tomato , #673770 , #008080 , #2F4F4F , #FAEBD7 , #ff1493 , #5e738f , #808000 , #D14285 , #ffa500 , cbd588 , wheat , #d2b48c , cyan2 , black , #BC8F8F , #800000 , #008B8B , #BC8F8F ) Then we can generate the following bar plot: plot_bar(actino , x= Class , fill= Order , facet_grid=~Well) + ylab( Relative abundance (%) ) + scale_fill_manual(values=set_colors)","title":"Subset and show the composition of the Actinobacteria"},{"location":"part5/#improving-our-first-bar-plot","text":"We'll extract the abundance data for the abundance phyla (e.g. Relative abundance 1%) and save the data student_data_prop_merged_prop_rank_filter - filter_taxa(student_data_prop_merged_prop_rank, function(x) mean(x) 1, TRUE) abun.phylum - psmelt(student_data_prop_merged_prop_rank_filter) View(abun.phylum) If everything looks good, we can save this dataset as a text file write.table(abun.phylum, output/phylum_over1percent.txt , sep= \\t , dec= . , col.names=NA) Then we'll calculate the sum of all remaining groups for an \"Others\" category. others - tax_glom(student_data_prop_merged_prop_rank_filter, Kingdom ) otu_table(others) - 100 - otu_table(others) tax_table(others)@.Data[,2:6] - Others # Define all taxanomic levels as Others taxa_names(others) - Others # Define the taxa name as Others Next we can combine the abundant taxa with the sum of rare taxa OTU1 - otu_table(cbind(otu_table(others), otu_table(student_data_prop_merged_prop_rank_filter) ), taxa_are_rows = FALSE) TAX1 - tax_table(rbind(tax_table(others), tax_table(student_data_prop_merged_prop_rank_filter) )) METADATA1 - sample_data(student_data_prop_merged_prop_rank_filter) final - phyloseq(OTU1, TAX1, METADATA1) Plot the abundance phyla using bar or heatmap plot_bar(final, fill=RANK) + scale_fill_manual(values=set_colors)+ theme(axis.text.x = element_text(angle = 0, vjust = 0.5, hjust = 0.5)) And we can save the figure using the ggplot function, \"ggsave\" ggsave( output/taxonomic composition_phylum.png , width = 4, height = 4) We could also generate a heatmap of the same data plot_heatmap(final, method = PCoA , distance = bray , sample.label= Well , taxa.label= Phylum , low= #000033 , high= #FF3300 )+ theme(axis.text.x = element_text(angle = 0, vjust = 0.5, hjust = 0.5)) # Note: by default plot_heatmap takes a log transformation (trans = log_trans(4)) ggsave( output/heatmap_phylum.png , width = 4, height = 4)","title":"Improving our first bar plot"},{"location":"part5/#extracting-the-top-30-otus-and-showing-the-abundance-across-samples","text":"top30 - prune_taxa(names(sort(taxa_sums(student_data_prop),TRUE)[1:30]), student_data_prop) plot_heatmap(top30, # method = NMDS , # default method method = NULL, # distance = bray , sample.label= Well , taxa.label= Class , trans = NULL)+ theme(axis.text.x = element_text(angle = -90, vjust = 0.5, hjust = 0.5)) ggsave( output/heatmap_top30.png , width = 4, height = 4)","title":"Extracting the top 30 OTUs and showing the abundance across samples"},{"location":"part5/#phylogenetic-tree","text":"We can use \"Ape\" and the provided phylogenetic tree to plot our abundant OTUs top20 - prune_taxa(names(sort(taxa_sums(student_data_prop_merged_prop),TRUE)[1:20]), student_data_prop_merged_prop) plot_tree(top20, color = Phylum , shape = Well , #label.tips = Family , # size = abundance , plot.margin = 0.5, ladderize = TRUE) + # coord_polar(theta= y )+ scale_shape_manual(values = c(1,2,16)) ggsave( output/phylo_tree_phylum.png , width = 4, height =4)","title":"Phylogenetic tree"},{"location":"part5/#exploring-alpha-diversity","text":"Many richness estimates are modeled on singletons and doubletons in the abundance data. You need to leave them in the dataset if you want a meaningful estimate. We'll be using the rarefied dataset we created in the \"Data Preprocessing steps\" above. student_data_rarefy - data_rarefy plot_richness(student_data_rarefy) # warnings probably because of the graphics that didn't show clearly p_alpha - plot_richness(student_data_rarefy, x= Well , measures=c( Simpson , Shannon ), color= Well ) + geom_point(size=5, alpha=0.7) + scale_color_manual(values = c( red , blue , black )) p_alpha","title":"Exploring Alpha Diversity"},{"location":"part5/#estimate-alpha-diversity","text":"alphadiv - estimate_richness(student_data_rarefy # , measures=c( Simpson , Shannon ) ) alphadiv","title":"estimate alpha diversity"},{"location":"part5/#phylogenetic-diversity","text":"Start off with a function that transposes the OTU matrix into a form that vegan can use. vegan_otu - function(physeq) { OTU - otu_table(physeq) if (taxa_are_rows(OTU)) { OTU - t(OTU) } return(as(OTU, matrix )) } To calculate Faith's phylogenetic distance we need a community data matrix and a phylo tree object FaithPD - pd(vegan_otu(student_data_rarefy), phy_tree(student_data_rarefy)) ## Returns a dataframe of the PD and species richness (SR) values for all samples alphadiv2 - data.frame(alphadiv, FaithPD) write.table(alphadiv2, output/alpha diversity.txt , sep= \\t , dec= . , col.names=NA) # Note: SR equals Observed using different functions identical(alphadiv2$SR, alphadiv2$Observed) Now you can go on with univariate data analysis, e.g. ANOVA or plot nice figures, For instance: # combine the diversity results with the sample data file sampledf - data.frame(sample_data(student_data_rarefy)) df - data.frame(alphadiv2, sampledf)","title":"Phylogenetic Diversity"},{"location":"part5/#plot-boxplot-with-significance","text":"Add significance levels of the diversity indices between different wells my_comparisons - list( c( H41 , H43 ), c( H43 , H52 ), c( H41 , H52 ) ) Phylogenetic diversity pd_well - ggboxplot(df, Well , PD , color = Well , palette =c( red , blue , black ), add = jitter , shape = Aquifer )+ stat_compare_means(comparisons = my_comparisons, label = p.signif ) + # pairwise comparison stat_compare_means(label.y = 50) # Add global p-value Shannon diversity shannon_well - ggboxplot(df, Well , Shannon , color = Well , palette =c( red , blue , black ), add = jitter , shape = Aquifer )+ stat_compare_means(comparisons = my_comparisons, label = p.signif ) + # pairwise comparison stat_compare_means(label.y = 5.5) # Add global p-value Plotting both together: grid.arrange(pd_well, shannon_well, ncol=2) dev.copy(png, file= alpha_diversity.png , width=1500, height=600, res=144) dev.off() #could also use ggsave() here","title":"Plot boxplot with significance"},{"location":"part5/#principal-component-analysis-of-geochemical-variables","text":"Read the data tables metadata_tb - read.csv( data/simulated_dataset.txt , sep= \\t , dec = . , row.names=1, header=TRUE) str(metadata_tb) # check internal structure of the metadata file metadata_tb.pca - prcomp(metadata_tb[,c(7:21)], center = TRUE, scale. = TRUE) # center = TRUE: a logical value indicating whether the variables should be shifted to be zero centered # center = TRUE, scale. = TRUE: the variables is adviced to be scaled to have unit variance before the analysis takes place To check the results of the PCA. The print method returns the standard deviation of each of the four PCs, and their rotation (or loadings), which are the coefficients of the linear combinations of the continuous variables. print(metadata_tb.pca) The summary method describe the importance of the PCs. We can see there that the first two PCs accounts for more than 51% of the variance of the data. summary(metadata_tb.pca)","title":"Principal Component Analysis of Geochemical Variables"},{"location":"part5/#plotting-with-base-r","text":"The plot method returns a plot of the variances (y-axis) associated with the PCs (x-axis). The Figure below is useful to decide how many PCs to retain for further analysis. We can see that the first two PCs explain most of the variability in the data. plot(metadata_tb.pca, type = l ) plot(metadata_tb.pca) A simple PCA plot biplot(metadata_tb.pca, choices = 1:2, scale = 1) Using ggbiplot ggbiplot(metadata_tb.pca) Nice looking plot ggbiplot(metadata_tb.pca, ellipse=TRUE, choices = 1:2, var.axes = TRUE, # Draw arrows for the variables? obs.scale = 1, var.scale = 1, labels=rownames(metadata_tb), groups=metadata_tb$Well) + scale_colour_manual(name= Well , values= c( red , blue , black ))+ ggtitle( PCA of simulated dataset )+ theme_bw()+ theme(legend.position = right ) ggsave( output/PCA of simulated dataset.png , width = 8, height = 6)","title":"Plotting with base R"},{"location":"part5/#beta-diversity-ordination-plots","text":"","title":"Beta Diversity Ordination Plots"},{"location":"part5/#clustering-and-dendrogram","text":"Calculate the distance matrix between samples dis - phyloseq::distance(student_data_prop, bray ) # distance calculation Hierarchical clustering hc1 - hclust(dis, method= average ) hc2 - hclust(dis, method= ward.D ) par(mfrow = c(2,1)) plot(hc1) plot(hc2) Add colors to the tips sampledf - data.frame(sample_data(student_data_prop)) well - sampledf$Well # palette -hue_pal()(length(levels(well))) palette - c( red , blue , black ) tipColoor - col_factor(palette, levels=levels(well))(well) Save it as phylo object clust.uf -as.phylo(hclust(dis,method= ward.D2 )) # png( output/cluster.png ) plot(clust.uf,tip.color=tipColoor, direction= downwards ) #10*20 dev.off() More info on different clustering methods: http://girke.bioinformatics.ucr.edu/GEN242/pages/mydoc/Rclustering.html","title":"Clustering and Dendrogram"},{"location":"part5/#unconstrained-ordination","text":"Perform an ordination on phyloseq data set.seed(123) NMDS plot based on Bray-Curtis distance ord - ordinate(student_data_prop, NMDS , bray ) plot ordination p = plot_ordination(student_data_prop, ord, # type= biplot , type= split , # type= samples , # type= taxa , shape= Well , color= Phylum , label= Well , title= NMDS based on Bray-Curtis distance )+ # geom_point(size=7, alpha=0.75)+ theme_bw() print(p) ggsave( output/NMDS_bray.png , width = 8, height = 4)","title":"Unconstrained ordination"},{"location":"part5/#pcoa-plot-based-on-unifrac-distance","text":"ordu = ordinate(student_data_prop, PCoA , unifrac , weighted=TRUE) summary(ordu) p1 = plot_ordination(student_data_prop, ordu, axes = 1:2, type= samples , shape= Season , color= Well , #label= Well , # type= taxa ,color= Phylum , title= PCoA based on weighted UniFrac distance )+ geom_point(size=7, alpha=0.75)+ scale_colour_manual(values= c( red , blue , black ))+ scale_shape_manual( values= c(0, 1,16,17))+ theme_bw() print(p1) remove the inner dots p1$layers -p1$layer [-1] print(p1) ggsave( output/PCoA_wunifrac.png , width = 8, height = 6) Scree plot: shows the fraction of total variance in the data as explained or represented by each PC. p_scree - plot_ordination(student_data_prop, ordu, type= scree ) print(p_scree)","title":"PCoA plot based on Unifrac distance"},{"location":"part5/#constrained-ordination","text":"","title":"Constrained ordination"},{"location":"part5/#rda","text":"ordu2 = ordinate(student_data_prop, RDA , unifrac , weighted=TRUE) summary(ordu2) p2 = plot_ordination(student_data_prop, ordu2, type= samples , # shape= Season , color= Well , #label= Well , # type= taxa ,color= Phylum , title= RDA based on weighted UniFrac distance )+ geom_point(size=7, alpha=0.75)+ theme_bw() print(p2) Remove the inner dots p2$layers -p2$layer [-1] print(p2) ggsave( output/RDA_wunifrac.png , width = 6, height = 4) Scree plot: shows the fraction of total variance in the data as explained or represented by each PC. p2_scree - plot_ordination(student_data_prop, ordu2, type= scree ) print(p2_scree)","title":"RDA"},{"location":"part5/#cca-plot-based-on-unifrac-distance","text":"set.seed(123) ordu3 = ordinate(student_data_prop, CCA , unifrac , weighted=TRUE) summary(ordu3) p3 = plot_ordination(student_data_prop, ordu3, axes = 1:2, type= samples , shape= Well , color= Well , #label= Well , # type= taxa ,color= Phylum , title= CCA based on weighted UniFrac distance )+ geom_point(size=7, alpha=0.75)+ theme_bw() print(p3) # remove the inner dots p3$layers -p3$layer [-1] print(p3) # scree plot: shows the fraction of total variance in the data as explained or represented by each PC. p3_scree - plot_ordination(student_data_prop, ordu3, type= scree ) print(p3_scree)","title":"ccA plot based on Unifrac distance"},{"location":"part5/#beta-diversity-hypothesis-testing","text":"","title":"Beta Diversity Hypothesis Testing"},{"location":"part5/#permanova-using-adonis-function-in-vegan-package","text":"First calculate bray curtis distance matrix using either weighted unifrac distance or Bray-Curtis distance # student_data_prop_bray - phyloseq::distance(student_data_prop, method = unifrac , weighted=TRUE) student_data_prop_bray - phyloseq::distance(student_data_prop, method = bray ) Make a data frame from the sample_data sampledf - data.frame(sample_data(student_data_prop)) head(sampledf) Then run the Adonis test (Number of permutations: 999) ht_well - adonis(student_data_prop_bray ~ Well, data = sampledf) # *** This output tells us that our adonis test is significant so we can reject the null hypothesis that our three countries have the same centroid. ht_well ht_well$aov.tab$ Pr( F) write.table(data.frame(ht_well$aov.tab), output/permanova_well.txt , sep= \\t , dec= . , col.names=NA)","title":"PERMANOVA using adonis function in Vegan package"},{"location":"part5/#posthoc-pairwise-adonis","text":"This is an R wrapper function for multilevel pairwise comparison using adonis (~Permanova) from package 'vegan'. The function returns adjusted p-values using p.adjust(). First, we need to convert the phyloseq object into an OTU-table vegan_otu - function(physeq) { OTU - otu_table(physeq) if (taxa_are_rows(OTU)) { OTU - t(OTU) } return(as(OTU, matrix )) } pairwise.adonis(vegan_otu(student_data_prop), sampledf$Well) !!! Your Turn Test if there was any effect of other factors on bacterial community composition, e.g. Season Hint: adonis(student_data_prop_bray ~ Season, data = sampledf) # This output tells us that our adonis test is non-significant You can do the rest....","title":"Posthoc Pairwise Adonis"},{"location":"part5/#analysis-of-similarities-using-anosim-function-in-vegan-package","text":"ano_well - anosim(student_data_prop_bray, sampledf$Well) summary(ano_well) par(mfrow=c(1,1)) plot(ano_well) You can test if there was any effect of other factors on bacterial community composition, e.g. Season ano_season - anosim(student_data_prop_bray, sampledf$Season)","title":"Analysis of Similarities using anosim function in Vegan package"},{"location":"part5/#homogeneity-of-dispersion-test-using-betadisper-function-in-vegan-package","text":"This is a nonmetric test based on permutations dis_well - betadisper(student_data_prop_bray, sampledf$Well) permutest(dis_well) # ** dis_season - betadisper(student_data_prop_bray, sampledf$Season) permutest(dis_season) # ns Additionally, our betadisper results are significant, meaning we cannot reject the null hypothesis that our groups have the same dispersions. This means we cannot be confident that our adonis result is a real result, and not due to differences in group dispersions","title":"Homogeneity of dispersion test using betadisper function in Vegan package"},{"location":"part5/#tukeys-honest-significant-differences","text":"well.HSD - TukeyHSD(dis_well) well.HSD Plot dispersion distances between groups par(mfrow=c(1,2)) plot(well.HSD, las=1) plot(dis_well, las=1)","title":"Tukey's Honest Significant Differences"},{"location":"part5/#linking-to-environmental-parameters-and-biplots","text":"We'll use OTUs with mean relative abundance over 0.01% for CCA analysis student_data_prop_filter - filter_taxa(student_data_prop, function(x) mean(x) 0.01, TRUE) Import the OTU table and geochemical data: otutable - vegan_otu(student_data_prop_filter) sampledf - data.frame(sample_data(student_data_prop_filter)) To check whether to choose CCA or RDA, we should use Detrended correspondence analysis (DCA) dca - decorana(otutable) dca Check the \"Axis lengths\" of the DCA1 (Important) summary(dca) # to check the Axis lengths of the DCA1 (Important) If this value 3, it is better to use RDA If this value 4, it is better to use CCA If this value is between 3 and 4, either use CCA or RDA","title":"Linking to environmental parameters and biplots"},{"location":"part5/#envfit-ccarda","text":"Much more powerful when more complex factors are tested","title":"Envfit + CCA/RDA"},{"location":"part5/#cca","text":"Scale species to unit variance df - data.frame(scale(sampledf[,-c(1:6)])) Test all environmental factors ord_all - cca(otutable ~ ., data=df) You can choose to test only some of the environmental factors ord_select - cca(otutable ~ EC + DO + PH, df) #","title":"CCA"},{"location":"part5/#rda_1","text":"bray - vegdist(otutable, bray ) ord_all - capscale(bray~., df]) Plot all environmental factors plot(ord_all, type = p , scaling = sites )","title":"RDA"},{"location":"part5/#variance-inflation-factors","text":"Linear dependencies between constraints can be investigated via the variance inflation factor or VIF VIF is a measure of how much the variance of $\\hat{\\beta}_j$ is inflated by presence of other covariates Lots of rules of thumb VIF = 20 indicates strong collinearity in constraints VIF = 10 potnetially of concern should be looked at They will be completely removed from the estimation, and no biplot scores or centroids are calculated for these aliased constraints. vif.cca(ord) temp - vif.cca(ord_all) temp select_para - names(temp[temp 10]) select_para Keep the environmental factors with VIF less than 10 ord - cca(otutable ~ ., df[,select_para])","title":"Variance Inflation Factors"},{"location":"part5/#fit-environmental-vectors-onto-the-ordination","text":"The function fits environmental vectors or factors onto an ordination. The projections of points onto vectors have maximum correlation with corresponding environmental variables, and the factors show the averages of factor levels. fit - envfit(ord, df[,select_para], perm = 999, display = lc , scaling = sites ) fit$vectors # check the significance Extract the best (significant) variables (p 0.05) spp.scrs - as.data.frame(scores(fit, display = vectors )) spp.scrs pval - fit$vectors$pvals pval Data for the envfit arrows This is necessary, see Gavin Simpson http://stackoverflow.com/questions/14711470/plotting-envfit-vectors-vegan-package-in-ggplot2/25425258#25425258 fdat - cbind(spp.scrs, Vector = rownames(spp.scrs), pval) Now select only the signifant factors bestEnvVariables -rownames(fdat)[fdat$pval =0.05] If you have NA entries, remove them # bestEnvVariables -bestEnvVariables[!is.na(bestEnvVariables)] Redo CCA using the best environmental variables eval(parse(text=paste( ord1 - cca(otutable ~ ,do.call(paste,c(as.list(bestEnvVariables),sep= + )), ,data=sampledf) ,sep= ))) summary(ord1) You can also use anova.cca to select the significant variables anova.cca(ord1, perm=9999) anova.cca(ord1, by= margin , perm=9999) # marginal effects of the terms anova.cca(ord1, by= terms , perm=9999) # sequential anova.cca(ord1, by= axis ) # axis, slow drop1(ord1, test= perm ) Now re-fit the environmental factors on ordination fit1 - envfit(ord1,sampledf[,bestEnvVariables], perm = 999, display = lc , scaling = sites ) Simple triplot plot(ord1) Simple biplot plot(ord1, type= n ) Plot the samples points(ord1, display = sites , col = as.numeric(Moisture), pch=16) Choose the significant environmental factors plot(fit1, col = red , cex=1.2, axis=TRUE, p.max = 0.05) summary(ord1)","title":"Fit environmental vectors onto the ordination"},{"location":"part5/#plot-using-ggplot2","text":"spp.scrs - data.frame(scores(fit1, display = vectors )) pval - fit1$vectors$pvals #data for the envfit arrows spp.scrs - cbind(spp.scrs, Vector = rownames(spp.scrs), pval) # vector table scrs - as.data.frame(scores(ord1, display = sites )) # sample table scrs - cbind(scrs, sampledf) spp.scrs1 - subset(spp.scrs, pval =0.05) #extracts relevant environment vectors from envifit Biplot library(digest) p - ggplot(scrs) + geom_point(mapping = aes(x = CCA1, y = CCA2, colour = Well), alpha = 0.8, size = 8) + #coord_fixed() + ## need aspect ratio of 1! geom_segment(data = spp.scrs1, aes(x = 0, xend = CCA1*2.5, y = 0, yend = CCA2*2.5), arrow = arrow(length = unit(0.25, cm )), colour = blue , size=1) + geom_text(data = spp.scrs1, aes(x = CCA1*3, y = CCA2*3, label = Vector), size = 4) + theme_bw() + theme(axis.title=element_text(size=14), axis.text=element_text(size=14), legend.text=element_text(size=14)) + #scale_color_manual(values = set_colors3, guide = guide_legend(ncol=1)) + scale_color_manual(values =c( red , blue , black ), guide = guide_legend(ncol=1)) + #xlim(-2,3)+ theme(legend.position= right ) p ggsave( output/cca_linking_env.png , width = 6, height = 4) Question ggplot: Triplot? Hint:extract species scores spe - scores(ord1, display = species ) tax - tax_table(student_data_prop_filter) otu - otu_table(student_data_prop_filter) abundance - rowMeans(x=otu)# calculate mean abundance of each OTU df2 - data.frame(spe, tax, Abundance = abundance) df2 - subset(df2, abundance =0.05) More info on model selection: https://github.com/naupaka/esa_vegan/blob/master/03-constrained-ordination/constrained-ordination.md","title":"Plot using ggplot2"},{"location":"part5/#bioindicators","text":"biocLite( DESeq2 ) biocLite( IRanges ) library(IRanges) library(DESeq2) library(ggpubr) For this analysis, we use the count data. because this method does its own normalization If we only want to identy the taxa that discriminate H43 and H52 student_data_well - subset_samples(student_data, Well!= H41 )","title":"Bioindicators"},{"location":"part5/#deseq2","text":"The following two lines actually do all the complicated DESeq2 work. The function phyloseq_to_deseq2 converts your phyloseq-format microbiome data into a DESeqDataSet with dispersions estimated using the experimental design formula, also shown (the ~Well term). dds = phyloseq_to_deseq2(student_data_well, ~ Well) Then DESeq function does the rest of the testing - estimation of size factors: estimateSizeFactors - estimation of dispersion: estimateDispersions - Negative Binomial GLM fitting and Wald statistics: nbinomWaldTest Wald statistics: nbinomWaldTest dds - DESeq(dds, test= Wald , fitType= parametric ) Negative Binomial GLM fitting and ddsLRT - DESeq(dds, test= LRT , reduced= ~ 1) Check our results res = results(dds, cooksCutoff = FALSE) alpha = 0.001 sigtab = res[which(res$padj alpha), ] # sigtab = sigtab[which(abs(sigtab$log2FoldChange) 2), ] # sometimes you want to report the taxa that had higher log2FoldChange than 2 sigtab = cbind(as(sigtab, data.frame ), as(tax_table(student_data_well)[rownames(sigtab), ], matrix )) head(sigtab)","title":"DESeq2"},{"location":"part5/#plot","text":"Phylum order x = tapply(sigtab$log2FoldChange, sigtab$Phylum, function(x) max(x)) x = sort(x, TRUE) sigtab$Phylum = factor(as.character(sigtab$Phylum), levels=names(x)) Genus order x = tapply(sigtab$log2FoldChange, sigtab$Genus, function(x) max(x)) x = sort(x, TRUE) sigtab$Genus = factor(as.character(sigtab$Genus), levels=names(x)) Ploting (Note: H43 is used as a control, FoldChange = H52/H43) sigtab$OTU - rownames(sigtab) ggbarplot(sigtab, x = OTU , y = log2FoldChange , fill = Phylum , # change fill color by mpg_level color = white , # Set bar border colors to white palette = jco , # jco journal color palett. see ?ggpar sort.val = asc , # Sort the value in ascending order sort.by.groups = FALSE, # Don't sort inside each group ylab = log2FoldChange , legend.title = Phylum , rotate = TRUE, ggtheme = theme_classic() ) ggsave( output/bioindicators.png , width = 6, height = 3) Show the abundance difference of the interesting taxa between the two wells select - rownames(sigtab) student_data_well_select - subset_taxa(student_data_well, rownames(tax_table(student_data_well)) %in% select[2]) plot_bar(student_data_well_select, Well , fill= Genus ) + ylab( Sum of normalized abundance ) + theme(axis.text.x = element_text(angle = -90, vjust = 0, hjust = 1)) ggsave( output/bioindicators_bar.png , width = 6, height = 3) plot_heatmap(student_data_well_select, sample.label= Well , taxa.label= Genus , taxa.order= Phylum , sample.order= Well )","title":"Plot"},{"location":"part5/#random-forest","text":"https://rstudio-pubs-static.s3.amazonaws.com/115631_7397b7cf67534479ae80f70546610eea.html Written by Lijuan Yan Oct 2018","title":"Random Forest"}]}